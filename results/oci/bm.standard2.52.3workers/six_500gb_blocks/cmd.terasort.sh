{ time /usr/bin/hadoop fs -rm -r -skipTrash /Terasort/Input; } 2>> terasort.bigdata.time.txt 1>> terasort.bigdata.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar teragen -Ddfs.replication=1 -Ddfs.client.block.write.locateFollowingBlock.retries=15 -Dyarn.app.mapreduce.am.job.cbd-mode.enable=false -Ddfs.blocksize=536870912 -Dmapreduce.map.java.opts="-Xms`expr 3 \* 778240 / 612 / 10 \* 8`m -Xmx`expr 3 \* 778240 / 612 / 10 \* 8`m" -Dyarn.app.mapreduce.am.job.map.pushdown=false -Dmapreduce.job.maps=612 -Dmapreduce.map.memory.mb=`expr 3 \* 778240 / 612` 6000000000 /Terasort/Input; } 2>> terasort.bigdata.time.txt 1>> terasort.bigdata.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Terasort/Output; } 2>> terasort.bigdata.time.txt 1>> terasort.bigdata.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar terasort -Dmapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.Shuffle -Ddfs.client.block_write.locateFollowingBlock.retries=30 -Dmapred.reduce.child.log.level=WARN -Ddfs.replication=1 -Ddfs.blocksize=536870912 -Dmapreduce.map.java.opts="-Xms`expr 3 \* 778240 / 612 / 10 \* 8`m -Xmx`expr 3 \* 778240 / 612 / 10 \* 8`m" -Dmapreduce.reduce.java.opts="-Xms`expr 3 \* 778240 / 612 / 10 \* 8`m -Xmx`expr 3 \* 778240 / 612 / 10 \* 8`m" -Dyarn.app.mapreduce.am.job.cbd-mode.enable=false -Dyarn.app.mapreduce.am.job.map.pushdown=false -Dmapreduce.job.maps=612 -Dmapreduce.job.reduces=612 -Dmapreduce.reduce.memory.mb=3814 /Terasort/Input /Terasort/Output; } 2>> terasort.bigdata.time.txt 1>> terasort.bigdata.out.txt
