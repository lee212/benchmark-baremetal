{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.16.1-tests.jar TestDFSIO -write -nrFiles 16 -fileSize 1 -resFile logs/dfsio.16.1 -bufferSize 4096; } 2>> dfsio.tiny.time.txt 1>> dfsio.tiny.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.16.1-tests.jar TestDFSIO -read -nrFiles 16 -fileSize 1 -resFile logs/dfsio.16.1 -bufferSize 4096; } 2>> dfsio.tiny.time.txt 1>> dfsio.tiny.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.16.1-tests.jar TestDFSIO -write -nrFiles 32 -fileSize 10 -resFile logs/dfsio.32.10 -bufferSize 4096; } 2>> dfsio.small.time.txt 1>> dfsio.small.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.16.1-tests.jar TestDFSIO -read -nrFiles 32 -fileSize 10 -resFile logs/dfsio.32.10 -bufferSize 4096; } 2>> dfsio.small.time.txt 1>> dfsio.small.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.16.1-tests.jar TestDFSIO -write -nrFiles 64 -fileSize 10 -resFile logs/dfsio.64.10 -bufferSize 4096; } 2>> dfsio.large.time.txt 1>> dfsio.large.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.16.1-tests.jar TestDFSIO -read -nrFiles 64 -fileSize 10 -resFile logs/dfsio.64.10 -bufferSize 4096; } 2>> dfsio.large.time.txt 1>> dfsio.large.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.16.1-tests.jar TestDFSIO -write -nrFiles 256 -fileSize 100 -resFile logs/dfsio.256.100 -bufferSize 4096; } 2>> dfsio.huge.time.txt 1>> dfsio.huge.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.16.1-tests.jar TestDFSIO -read -nrFiles 256 -fileSize 100 -resFile logs/dfsio.256.100 -bufferSize 4096; } 2>> dfsio.huge.time.txt 1>> dfsio.huge.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.16.1-tests.jar TestDFSIO -write -nrFiles 512 -fileSize 400 -resFile logs/dfsio.512.400 -bufferSize 4096; } 2>> dfsio.gigantic.time.txt 1>> dfsio.gigantic.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.16.1-tests.jar TestDFSIO -read -nrFiles 512 -fileSize 400 -resFile logs/dfsio.512.400 -bufferSize 4096; } 2>> dfsio.gigantic.time.txt 1>> dfsio.gigantic.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.16.1-tests.jar TestDFSIO -write -nrFiles 2048 -fileSize 1000 -resFile logs/dfsio.2048.1000 -bufferSize 4096; } 2>> dfsio.bigdata.time.txt 1>> dfsio.bigdata.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.16.1-tests.jar TestDFSIO -read -nrFiles 2048 -fileSize 1000 -resFile logs/dfsio.2048.1000 -bufferSize 4096; } 2>> dfsio.bigdata.time.txt 1>> dfsio.bigdata.out.txt
{ time hadoop fs -rm -r -skipTrash /Kmeans/Input; } 2>> kmeans.tiny.time.txt 1>> kmeans.tiny.out.txt
{ time hadoop jar /var/lib/hadoop-hdfs/autogen-7.1-SNAPSHOT-jar-with-dependencies.jar org.apache.mahout.clustering.kmeans.GenKMeansDataset -D hadoop.job.history.user.location=/Kmeans/Input/samples -sampleDir /Kmeans/Input/samples -clusterDir /Kmeans/Input/cluster -numClusters 5 -numSamples 30000 -samplesPerFile 6000 -sampleDimension 3; } 2>> kmeans.tiny.time.txt 1>> kmeans.tiny.out.txt
{ time hadoop fs -rm -r -skipTrash /Kmeans/Output; } 2>> kmeans.tiny.time.txt 1>> kmeans.tiny.out.txt
{ time mahout kmeans --input /Kmeans/Input/samples --output /Kmeans/Output --clusters /Kmeans/Input/cluster --maxIter 5 --overwrite --clustering --convergenceDelta 0.5 --distanceMeasure  org.apache.mahout.common.distance.EuclideanDistanceMeasure --method mapreduce; } 2>> kmeans.tiny.time.txt 1>> kmeans.tiny.out.txt
{ time hadoop fs -rm -r -skipTrash /Kmeans/Input; } 2>> kmeans.small.time.txt 1>> kmeans.small.out.txt
{ time hadoop jar /var/lib/hadoop-hdfs/autogen-7.1-SNAPSHOT-jar-with-dependencies.jar org.apache.mahout.clustering.kmeans.GenKMeansDataset -D hadoop.job.history.user.location=/Kmeans/Input/samples -sampleDir /Kmeans/Input/samples -clusterDir /Kmeans/Input/cluster -numClusters 5 -numSamples 3000000 -samplesPerFile 600000 -sampleDimension 20; } 2>> kmeans.small.time.txt 1>> kmeans.small.out.txt
{ time hadoop fs -rm -r -skipTrash /Kmeans/Output; } 2>> kmeans.small.time.txt 1>> kmeans.small.out.txt
{ time mahout kmeans --input /Kmeans/Input/samples --output /Kmeans/Output --clusters /Kmeans/Input/cluster --maxIter 5 --overwrite --clustering --convergenceDelta 0.5 --distanceMeasure  org.apache.mahout.common.distance.EuclideanDistanceMeasure --method mapreduce; } 2>> kmeans.small.time.txt 1>> kmeans.small.out.txt
{ time hadoop fs -rm -r -skipTrash /Kmeans/Input; } 2>> kmeans.large.time.txt 1>> kmeans.large.out.txt
{ time hadoop jar /var/lib/hadoop-hdfs/autogen-7.1-SNAPSHOT-jar-with-dependencies.jar org.apache.mahout.clustering.kmeans.GenKMeansDataset -D hadoop.job.history.user.location=/Kmeans/Input/samples -sampleDir /Kmeans/Input/samples -clusterDir /Kmeans/Input/cluster -numClusters 5 -numSamples 20000000 -samplesPerFile 4000000 -sampleDimension 20; } 2>> kmeans.large.time.txt 1>> kmeans.large.out.txt
{ time hadoop fs -rm -r -skipTrash /Kmeans/Output; } 2>> kmeans.large.time.txt 1>> kmeans.large.out.txt
{ time mahout kmeans --input /Kmeans/Input/samples --output /Kmeans/Output --clusters /Kmeans/Input/cluster --maxIter 5 --overwrite --clustering --convergenceDelta 0.5 --distanceMeasure  org.apache.mahout.common.distance.EuclideanDistanceMeasure --method mapreduce; } 2>> kmeans.large.time.txt 1>> kmeans.large.out.txt
{ time hadoop fs -rm -r -skipTrash /Kmeans/Input; } 2>> kmeans.huge.time.txt 1>> kmeans.huge.out.txt
{ time hadoop jar /var/lib/hadoop-hdfs/autogen-7.1-SNAPSHOT-jar-with-dependencies.jar org.apache.mahout.clustering.kmeans.GenKMeansDataset -D hadoop.job.history.user.location=/Kmeans/Input/samples -sampleDir /Kmeans/Input/samples -clusterDir /Kmeans/Input/cluster -numClusters 5 -numSamples 100000000 -samplesPerFile 20000000 -sampleDimension 20; } 2>> kmeans.huge.time.txt 1>> kmeans.huge.out.txt
{ time hadoop fs -rm -r -skipTrash /Kmeans/Output; } 2>> kmeans.huge.time.txt 1>> kmeans.huge.out.txt
{ time mahout kmeans --input /Kmeans/Input/samples --output /Kmeans/Output --clusters /Kmeans/Input/cluster --maxIter 5 --overwrite --clustering --convergenceDelta 0.5 --distanceMeasure  org.apache.mahout.common.distance.EuclideanDistanceMeasure --method mapreduce; } 2>> kmeans.huge.time.txt 1>> kmeans.huge.out.txt
{ time hadoop fs -rm -r -skipTrash /Kmeans/Input; } 2>> kmeans.gigantic.time.txt 1>> kmeans.gigantic.out.txt
{ time hadoop jar /var/lib/hadoop-hdfs/autogen-7.1-SNAPSHOT-jar-with-dependencies.jar org.apache.mahout.clustering.kmeans.GenKMeansDataset -D hadoop.job.history.user.location=/Kmeans/Input/samples -sampleDir /Kmeans/Input/samples -clusterDir /Kmeans/Input/cluster -numClusters 5 -numSamples 200000000 -samplesPerFile 40000000 -sampleDimension 20; } 2>> kmeans.gigantic.time.txt 1>> kmeans.gigantic.out.txt
{ time hadoop fs -rm -r -skipTrash /Kmeans/Output; } 2>> kmeans.gigantic.time.txt 1>> kmeans.gigantic.out.txt
{ time mahout kmeans --input /Kmeans/Input/samples --output /Kmeans/Output --clusters /Kmeans/Input/cluster --maxIter 5 --overwrite --clustering --convergenceDelta 0.5 --distanceMeasure  org.apache.mahout.common.distance.EuclideanDistanceMeasure --method mapreduce; } 2>> kmeans.gigantic.time.txt 1>> kmeans.gigantic.out.txt
{ time hadoop fs -rm -r -skipTrash /Kmeans/Input; } 2>> kmeans.bigdata.time.txt 1>> kmeans.bigdata.out.txt
{ time hadoop jar /var/lib/hadoop-hdfs/autogen-7.1-SNAPSHOT-jar-with-dependencies.jar org.apache.mahout.clustering.kmeans.GenKMeansDataset -D hadoop.job.history.user.location=/Kmeans/Input/samples -sampleDir /Kmeans/Input/samples -clusterDir /Kmeans/Input/cluster -numClusters 5 -numSamples 1200000000 -samplesPerFile 40000000 -sampleDimension 20; } 2>> kmeans.bigdata.time.txt 1>> kmeans.bigdata.out.txt
{ time hadoop fs -rm -r -skipTrash /Kmeans/Output; } 2>> kmeans.bigdata.time.txt 1>> kmeans.bigdata.out.txt
{ time mahout kmeans --input /Kmeans/Input/samples --output /Kmeans/Output --clusters /Kmeans/Input/cluster --maxIter 10 --overwrite --clustering --convergenceDelta 0.5 --distanceMeasure  org.apache.mahout.common.distance.EuclideanDistanceMeasure --method mapreduce; } 2>> kmeans.bigdata.time.txt 1>> kmeans.bigdata.out.txt
{ time hadoop fs -rm -r -skipTrash /Pagerank/Input; } 2>> pagerank.tiny.time.txt 1>> pagerank.tiny.out.txt
{ time hadoop jar /var/lib/hadoop-hdfs/autogen-7.1-SNAPSHOT-jar-with-dependencies.jar HiBench.DataGen -t pagerank -b /Pagerank -n Input -m 8 -r 8 -p 50 -pbalance -pbalance -o text; } 2>> pagerank.tiny.time.txt 1>> pagerank.tiny.out.txt
{ time hadoop fs -rm -r -skipTrash /Pagerank/Output; } 2>> pagerank.tiny.time.txt 1>> pagerank.tiny.out.txt
{ time hadoop jar /var/lib/hadoop-hdfs/pegasus-2.0-SNAPSHOT.jar pegasus.PagerankNaive /Pagerank/Input/edges /Pagerank/Output 50 8 1 nosym new; } 2>> pagerank.tiny.time.txt 1>> pagerank.tiny.out.txt
{ time hadoop fs -rm -r -skipTrash /Pagerank/Input; } 2>> pagerank.small.time.txt 1>> pagerank.small.out.txt
{ time hadoop jar /var/lib/hadoop-hdfs/autogen-7.1-SNAPSHOT-jar-with-dependencies.jar HiBench.DataGen -t pagerank -b /Pagerank -n Input -m 204 -r 204 -p 5000 -pbalance -pbalance -o text; } 2>> pagerank.small.time.txt 1>> pagerank.small.out.txt
{ time hadoop fs -rm -r -skipTrash /Pagerank/Output; } 2>> pagerank.small.time.txt 1>> pagerank.small.out.txt
{ time hadoop jar /var/lib/hadoop-hdfs/pegasus-2.0-SNAPSHOT.jar pegasus.PagerankNaive /Pagerank/Input/edges /Pagerank/Output 5000 204 1 nosym new; } 2>> pagerank.small.time.txt 1>> pagerank.small.out.txt
{ time hadoop fs -rm -r -skipTrash /Pagerank/Input; } 2>> pagerank.large.time.txt 1>> pagerank.large.out.txt
{ time hadoop jar /var/lib/hadoop-hdfs/autogen-7.1-SNAPSHOT-jar-with-dependencies.jar HiBench.DataGen -t pagerank -b /Pagerank -n Input -m 204 -r 204 -p 500000 -pbalance -pbalance -o text; } 2>> pagerank.large.time.txt 1>> pagerank.large.out.txt
{ time hadoop fs -rm -r -skipTrash /Pagerank/Output; } 2>> pagerank.large.time.txt 1>> pagerank.large.out.txt
{ time hadoop jar /var/lib/hadoop-hdfs/pegasus-2.0-SNAPSHOT.jar pegasus.PagerankNaive /Pagerank/Input/edges /Pagerank/Output 500000 204 1 nosym new; } 2>> pagerank.large.time.txt 1>> pagerank.large.out.txt
{ time hadoop fs -rm -r -skipTrash /Pagerank/Input; } 2>> pagerank.huge.time.txt 1>> pagerank.huge.out.txt
{ time hadoop jar /var/lib/hadoop-hdfs/autogen-7.1-SNAPSHOT-jar-with-dependencies.jar HiBench.DataGen -t pagerank -b /Pagerank -n Input -m 204 -r 204 -p 5000000 -pbalance -pbalance -o text; } 2>> pagerank.huge.time.txt 1>> pagerank.huge.out.txt
{ time hadoop fs -rm -r -skipTrash /Pagerank/Output; } 2>> pagerank.huge.time.txt 1>> pagerank.huge.out.txt
{ time hadoop jar /var/lib/hadoop-hdfs/pegasus-2.0-SNAPSHOT.jar pegasus.PagerankNaive /Pagerank/Input/edges /Pagerank/Output 5000000 204 1 nosym new; } 2>> pagerank.huge.time.txt 1>> pagerank.huge.out.txt
{ time hadoop fs -rm -r -skipTrash /Pagerank/Input; } 2>> pagerank.gigantic.time.txt 1>> pagerank.gigantic.out.txt
{ time hadoop jar /var/lib/hadoop-hdfs/autogen-7.1-SNAPSHOT-jar-with-dependencies.jar HiBench.DataGen -t pagerank -b /Pagerank -n Input -m 204 -r 204 -p 30000000 -pbalance -pbalance -o text; } 2>> pagerank.gigantic.time.txt 1>> pagerank.gigantic.out.txt
{ time hadoop fs -rm -r -skipTrash /Pagerank/Output; } 2>> pagerank.gigantic.time.txt 1>> pagerank.gigantic.out.txt
{ time hadoop jar /var/lib/hadoop-hdfs/pegasus-2.0-SNAPSHOT.jar pegasus.PagerankNaive /Pagerank/Input/edges /Pagerank/Output 30000000 204 1 nosym new; } 2>> pagerank.gigantic.time.txt 1>> pagerank.gigantic.out.txt
{ time hadoop fs -rm -r -skipTrash /Pagerank/Input; } 2>> pagerank.bigdata.time.txt 1>> pagerank.bigdata.out.txt
{ time hadoop jar /var/lib/hadoop-hdfs/autogen-7.1-SNAPSHOT-jar-with-dependencies.jar HiBench.DataGen -t pagerank -b /Pagerank -n Input -m 204 -r 204 -p 50000000 -pbalance -pbalance -o text; } 2>> pagerank.bigdata.time.txt 1>> pagerank.bigdata.out.txt
{ time hadoop fs -rm -r -skipTrash /Pagerank/Output; } 2>> pagerank.bigdata.time.txt 1>> pagerank.bigdata.out.txt
{ time hadoop jar /var/lib/hadoop-hdfs/pegasus-2.0-SNAPSHOT.jar pegasus.PagerankNaive /Pagerank/Input/edges /Pagerank/Output 50000000 204 1 nosym new; } 2>> pagerank.bigdata.time.txt 1>> pagerank.bigdata.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Terasort/Input; } 2>> terasort.tiny.time.txt 1>> terasort.tiny.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar teragen -Ddfs.replication=1 -Ddfs.client.block.write.locateFollowingBlock.retries=15 -Dyarn.app.mapreduce.am.job.cbd-mode.enable=false -Ddfs.blocksize=536870912 -Dmapreduce.map.java.opts=-Xms3048m -Xmx3048m -Dyarn.app.mapreduce.am.job.map.pushdown=false -Dmapreduce.job.maps=612 -Dmapreduce.map.memory.mb=3814 32000 /Terasort/Input; } 2>> terasort.tiny.time.txt 1>> terasort.tiny.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Terasort/Output; } 2>> terasort.tiny.time.txt 1>> terasort.tiny.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar terasort -Dmapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.Shuffle -Ddfs.client.block_write.locateFollowingBlock.retries=30 -Dmapred.reduce.child.log.level=WARN -Ddfs.replication=1 -Ddfs.blocksize=536870912 -Dmapreduce.map.java.opts=-Xms3048m -Xmx3048m -Dmapreduce.reduce.java.opts=-Xms3048m -Xmx3048m -Dyarn.app.mapreduce.am.job.cbd-mode.enable=false -Dyarn.app.mapreduce.am.job.map.pushdown=false -Dmapreduce.job.maps=612 -Dmapreduce.job.reduces=612 -Dmapreduce.reduce.memory.mb=3814 /Terasort/Input /Terasort/Output; } 2>> terasort.tiny.time.txt 1>> terasort.tiny.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Terasort/Input; } 2>> terasort.small.time.txt 1>> terasort.small.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar teragen -Ddfs.replication=1 -Ddfs.client.block.write.locateFollowingBlock.retries=15 -Dyarn.app.mapreduce.am.job.cbd-mode.enable=false -Ddfs.blocksize=536870912 -Dmapreduce.map.java.opts=-Xms3048m -Xmx3048m -Dyarn.app.mapreduce.am.job.map.pushdown=false -Dmapreduce.job.maps=612 -Dmapreduce.map.memory.mb=3814 3200000 /Terasort/Input; } 2>> terasort.small.time.txt 1>> terasort.small.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Terasort/Output; } 2>> terasort.small.time.txt 1>> terasort.small.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar terasort -Dmapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.Shuffle -Ddfs.client.block_write.locateFollowingBlock.retries=30 -Dmapred.reduce.child.log.level=WARN -Ddfs.replication=1 -Ddfs.blocksize=536870912 -Dmapreduce.map.java.opts=-Xms3048m -Xmx3048m -Dmapreduce.reduce.java.opts=-Xms3048m -Xmx3048m -Dyarn.app.mapreduce.am.job.cbd-mode.enable=false -Dyarn.app.mapreduce.am.job.map.pushdown=false -Dmapreduce.job.maps=612 -Dmapreduce.job.reduces=612 -Dmapreduce.reduce.memory.mb=3814 /Terasort/Input /Terasort/Output; } 2>> terasort.small.time.txt 1>> terasort.small.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Terasort/Input; } 2>> terasort.large.time.txt 1>> terasort.large.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar teragen -Ddfs.replication=1 -Ddfs.client.block.write.locateFollowingBlock.retries=15 -Dyarn.app.mapreduce.am.job.cbd-mode.enable=false -Ddfs.blocksize=536870912 -Dmapreduce.map.java.opts=-Xms3048m -Xmx3048m -Dyarn.app.mapreduce.am.job.map.pushdown=false -Dmapreduce.job.maps=612 -Dmapreduce.map.memory.mb=3814 32000000 /Terasort/Input; } 2>> terasort.large.time.txt 1>> terasort.large.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Terasort/Output; } 2>> terasort.large.time.txt 1>> terasort.large.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar terasort -Dmapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.Shuffle -Ddfs.client.block_write.locateFollowingBlock.retries=30 -Dmapred.reduce.child.log.level=WARN -Ddfs.replication=1 -Ddfs.blocksize=536870912 -Dmapreduce.map.java.opts=-Xms3048m -Xmx3048m -Dmapreduce.reduce.java.opts=-Xms3048m -Xmx3048m -Dyarn.app.mapreduce.am.job.cbd-mode.enable=false -Dyarn.app.mapreduce.am.job.map.pushdown=false -Dmapreduce.job.maps=612 -Dmapreduce.job.reduces=612 -Dmapreduce.reduce.memory.mb=3814 /Terasort/Input /Terasort/Output; } 2>> terasort.large.time.txt 1>> terasort.large.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Terasort/Input; } 2>> terasort.huge.time.txt 1>> terasort.huge.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar teragen -Ddfs.replication=1 -Ddfs.client.block.write.locateFollowingBlock.retries=15 -Dyarn.app.mapreduce.am.job.cbd-mode.enable=false -Ddfs.blocksize=536870912 -Dmapreduce.map.java.opts=-Xms3048m -Xmx3048m -Dyarn.app.mapreduce.am.job.map.pushdown=false -Dmapreduce.job.maps=612 -Dmapreduce.map.memory.mb=3814 320000000 /Terasort/Input; } 2>> terasort.huge.time.txt 1>> terasort.huge.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Terasort/Output; } 2>> terasort.huge.time.txt 1>> terasort.huge.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar terasort -Dmapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.Shuffle -Ddfs.client.block_write.locateFollowingBlock.retries=30 -Dmapred.reduce.child.log.level=WARN -Ddfs.replication=1 -Ddfs.blocksize=536870912 -Dmapreduce.map.java.opts=-Xms3048m -Xmx3048m -Dmapreduce.reduce.java.opts=-Xms3048m -Xmx3048m -Dyarn.app.mapreduce.am.job.cbd-mode.enable=false -Dyarn.app.mapreduce.am.job.map.pushdown=false -Dmapreduce.job.maps=612 -Dmapreduce.job.reduces=612 -Dmapreduce.reduce.memory.mb=3814 /Terasort/Input /Terasort/Output; } 2>> terasort.huge.time.txt 1>> terasort.huge.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Terasort/Input; } 2>> terasort.gigantic.time.txt 1>> terasort.gigantic.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar teragen -Ddfs.replication=1 -Ddfs.client.block.write.locateFollowingBlock.retries=15 -Dyarn.app.mapreduce.am.job.cbd-mode.enable=false -Ddfs.blocksize=536870912 -Dmapreduce.map.java.opts=-Xms3048m -Xmx3048m -Dyarn.app.mapreduce.am.job.map.pushdown=false -Dmapreduce.job.maps=612 -Dmapreduce.map.memory.mb=3814 3200000000 /Terasort/Input; } 2>> terasort.gigantic.time.txt 1>> terasort.gigantic.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Terasort/Output; } 2>> terasort.gigantic.time.txt 1>> terasort.gigantic.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar terasort -Dmapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.Shuffle -Ddfs.client.block_write.locateFollowingBlock.retries=30 -Dmapred.reduce.child.log.level=WARN -Ddfs.replication=1 -Ddfs.blocksize=536870912 -Dmapreduce.map.java.opts=-Xms3048m -Xmx3048m -Dmapreduce.reduce.java.opts=-Xms3048m -Xmx3048m -Dyarn.app.mapreduce.am.job.cbd-mode.enable=false -Dyarn.app.mapreduce.am.job.map.pushdown=false -Dmapreduce.job.maps=612 -Dmapreduce.job.reduces=612 -Dmapreduce.reduce.memory.mb=3814 /Terasort/Input /Terasort/Output; } 2>> terasort.gigantic.time.txt 1>> terasort.gigantic.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Terasort/Input; } 2>> terasort.bigdata.time.txt 1>> terasort.bigdata.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar teragen -Ddfs.replication=1 -Ddfs.client.block.write.locateFollowingBlock.retries=15 -Dyarn.app.mapreduce.am.job.cbd-mode.enable=false -Ddfs.blocksize=536870912 -Dmapreduce.map.java.opts=-Xms3048m -Xmx3048m -Dyarn.app.mapreduce.am.job.map.pushdown=false -Dmapreduce.job.maps=612 -Dmapreduce.map.memory.mb=3814 6000000000 /Terasort/Input; } 2>> terasort.bigdata.time.txt 1>> terasort.bigdata.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Terasort/Output; } 2>> terasort.bigdata.time.txt 1>> terasort.bigdata.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar terasort -Dmapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.Shuffle -Ddfs.client.block_write.locateFollowingBlock.retries=30 -Dmapred.reduce.child.log.level=WARN -Ddfs.replication=1 -Ddfs.blocksize=536870912 -Dmapreduce.map.java.opts=-Xms3048m -Xmx3048m -Dmapreduce.reduce.java.opts=-Xms3048m -Xmx3048m -Dyarn.app.mapreduce.am.job.cbd-mode.enable=false -Dyarn.app.mapreduce.am.job.map.pushdown=false -Dmapreduce.job.maps=612 -Dmapreduce.job.reduces=612 -Dmapreduce.reduce.memory.mb=3814 /Terasort/Input /Terasort/Output; } 2>> terasort.bigdata.time.txt 1>> terasort.bigdata.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Wordcount/Input; } 2>> wordcount.tiny.time.txt 1>> wordcount.tiny.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar randomtextwriter -D mapreduce.randomtextwriter.totalbytes=32000 -D mapreduce.randomtextwriter.bytespermap=4000 /Wordcount/Input; } 2>> wordcount.tiny.time.txt 1>> wordcount.tiny.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Wordcount/Output; } 2>> wordcount.tiny.time.txt 1>> wordcount.tiny.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount /Wordcount/Input /Wordcount/Output; } 2>> wordcount.tiny.time.txt 1>> wordcount.tiny.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Wordcount/Input; } 2>> wordcount.small.time.txt 1>> wordcount.small.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar randomtextwriter -D mapreduce.randomtextwriter.totalbytes=320000000 -D mapreduce.randomtextwriter.bytespermap=40000000 /Wordcount/Input; } 2>> wordcount.small.time.txt 1>> wordcount.small.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Wordcount/Output; } 2>> wordcount.small.time.txt 1>> wordcount.small.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount /Wordcount/Input /Wordcount/Output; } 2>> wordcount.small.time.txt 1>> wordcount.small.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Wordcount/Input; } 2>> wordcount.large.time.txt 1>> wordcount.large.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar randomtextwriter -D mapreduce.randomtextwriter.totalbytes=3200000000 -D mapreduce.randomtextwriter.bytespermap=400000000 /Wordcount/Input; } 2>> wordcount.large.time.txt 1>> wordcount.large.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Wordcount/Output; } 2>> wordcount.large.time.txt 1>> wordcount.large.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount /Wordcount/Input /Wordcount/Output; } 2>> wordcount.large.time.txt 1>> wordcount.large.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Wordcount/Input; } 2>> wordcount.huge.time.txt 1>> wordcount.huge.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar randomtextwriter -D mapreduce.randomtextwriter.totalbytes=32000000000 -D mapreduce.randomtextwriter.bytespermap=4000000000 /Wordcount/Input; } 2>> wordcount.huge.time.txt 1>> wordcount.huge.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Wordcount/Output; } 2>> wordcount.huge.time.txt 1>> wordcount.huge.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount /Wordcount/Input /Wordcount/Output; } 2>> wordcount.huge.time.txt 1>> wordcount.huge.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Wordcount/Input; } 2>> wordcount.gigantic.time.txt 1>> wordcount.gigantic.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar randomtextwriter -D mapreduce.randomtextwriter.totalbytes=320000000000 -D mapreduce.randomtextwriter.bytespermap=40000000000 /Wordcount/Input; } 2>> wordcount.gigantic.time.txt 1>> wordcount.gigantic.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Wordcount/Output; } 2>> wordcount.gigantic.time.txt 1>> wordcount.gigantic.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount /Wordcount/Input /Wordcount/Output; } 2>> wordcount.gigantic.time.txt 1>> wordcount.gigantic.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Wordcount/Input; } 2>> wordcount.bigdata.time.txt 1>> wordcount.bigdata.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar randomtextwriter -D mapreduce.randomtextwriter.totalbytes=1600000000000 -D mapreduce.randomtextwriter.bytespermap=200000000000 /Wordcount/Input; } 2>> wordcount.bigdata.time.txt 1>> wordcount.bigdata.out.txt
{ time /usr/bin/hadoop fs -rm -r -skipTrash /Wordcount/Output; } 2>> wordcount.bigdata.time.txt 1>> wordcount.bigdata.out.txt
{ time /usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount /Wordcount/Input /Wordcount/Output; } 2>> wordcount.bigdata.time.txt 1>> wordcount.bigdata.out.txt
