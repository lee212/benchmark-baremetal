rm: `/Join/Input': No such file or directory
18/12/17 22:12:39 INFO HiBench.HiveData: Generating hive data files...
18/12/17 22:12:39 INFO HiBench.HiveData: Initializing hive date generator...
18/12/17 22:12:41 INFO HiBench.Dummy: Creating dummy file /Join/temp/dummy with 156 slots...
18/12/17 22:12:42 INFO HiBench.HiveData: Creating table rankings...
18/12/17 22:12:42 INFO Configuration.deprecation: mapred.reduce.slowstart.completed.maps is deprecated. Instead, use mapreduce.job.reduce.slowstart.completedmaps
18/12/17 22:12:42 INFO HiBench.HiveData: Running Job: Create rankings
18/12/17 22:12:42 INFO HiBench.HiveData: Pages file /Join/temp/dummy as input
18/12/17 22:12:42 INFO HiBench.HiveData: Rankings file /Join/Input/rankings as output
18/12/17 22:12:42 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
18/12/17 22:12:42 INFO mapred.FileInputFormat: Total input paths to process : 1
18/12/17 22:12:42 INFO mapreduce.JobSubmitter: number of splits:156
18/12/17 22:12:43 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1545055483525_0061
18/12/17 22:12:43 INFO impl.YarnClientImpl: Submitted application application_1545055483525_0061
18/12/17 22:12:43 INFO mapreduce.Job: The url to track the job: http://cdh-master-1.private1.cdhvcn.oraclevcn.com:8088/proxy/application_1545055483525_0061/
18/12/17 22:12:43 INFO mapreduce.Job: Running job: job_1545055483525_0061
18/12/17 22:12:50 INFO mapreduce.Job: Job job_1545055483525_0061 running in uber mode : false
18/12/17 22:12:50 INFO mapreduce.Job:  map 0% reduce 0%
18/12/17 22:13:17 INFO mapreduce.Job:  map 1% reduce 0%
18/12/17 22:13:20 INFO mapreduce.Job:  map 2% reduce 0%
18/12/17 22:13:21 INFO mapreduce.Job:  map 6% reduce 0%
18/12/17 22:13:22 INFO mapreduce.Job:  map 13% reduce 0%
18/12/17 22:13:23 INFO mapreduce.Job:  map 18% reduce 0%
18/12/17 22:13:24 INFO mapreduce.Job:  map 22% reduce 0%
18/12/17 22:13:26 INFO mapreduce.Job:  map 23% reduce 0%
18/12/17 22:13:34 INFO mapreduce.Job:  map 24% reduce 0%
18/12/17 22:13:36 INFO mapreduce.Job:  map 27% reduce 0%
18/12/17 22:13:37 INFO mapreduce.Job:  map 37% reduce 0%
18/12/17 22:13:38 INFO mapreduce.Job:  map 55% reduce 0%
18/12/17 22:13:39 INFO mapreduce.Job:  map 67% reduce 0%
18/12/17 22:14:55 INFO mapreduce.Job:  map 68% reduce 0%
18/12/17 22:15:01 INFO mapreduce.Job:  map 69% reduce 0%
18/12/17 22:15:04 INFO mapreduce.Job:  map 70% reduce 0%
18/12/17 22:15:05 INFO mapreduce.Job:  map 71% reduce 0%
18/12/17 22:15:06 INFO mapreduce.Job:  map 73% reduce 0%
18/12/17 22:15:08 INFO mapreduce.Job:  map 74% reduce 0%
18/12/17 22:15:09 INFO mapreduce.Job:  map 75% reduce 0%
18/12/17 22:15:10 INFO mapreduce.Job:  map 76% reduce 0%
18/12/17 22:15:11 INFO mapreduce.Job:  map 77% reduce 0%
18/12/17 22:15:13 INFO mapreduce.Job:  map 78% reduce 0%
18/12/17 22:15:36 INFO mapreduce.Job:  map 78% reduce 1%
18/12/17 22:15:38 INFO mapreduce.Job:  map 78% reduce 3%
18/12/17 22:15:39 INFO mapreduce.Job:  map 78% reduce 5%
18/12/17 22:15:40 INFO mapreduce.Job:  map 78% reduce 8%
18/12/17 22:15:41 INFO mapreduce.Job:  map 78% reduce 11%
18/12/17 22:15:57 INFO mapreduce.Job:  map 79% reduce 11%
18/12/17 22:15:59 INFO mapreduce.Job:  map 80% reduce 12%
18/12/17 22:16:00 INFO mapreduce.Job:  map 82% reduce 12%
18/12/17 22:16:01 INFO mapreduce.Job:  map 83% reduce 13%
18/12/17 22:16:02 INFO mapreduce.Job:  map 85% reduce 13%
18/12/17 22:16:03 INFO mapreduce.Job:  map 86% reduce 14%
18/12/17 22:16:04 INFO mapreduce.Job:  map 88% reduce 15%
18/12/17 22:16:05 INFO mapreduce.Job:  map 90% reduce 18%
18/12/17 22:16:06 INFO mapreduce.Job:  map 93% reduce 20%
18/12/17 22:16:07 INFO mapreduce.Job:  map 95% reduce 21%
18/12/17 22:16:08 INFO mapreduce.Job:  map 97% reduce 22%
18/12/17 22:16:09 INFO mapreduce.Job:  map 99% reduce 24%
18/12/17 22:16:10 INFO mapreduce.Job:  map 99% reduce 26%
18/12/17 22:16:11 INFO mapreduce.Job:  map 100% reduce 29%
18/12/17 22:16:12 INFO mapreduce.Job:  map 100% reduce 32%
18/12/17 22:16:28 INFO mapreduce.Job:  map 100% reduce 34%
18/12/17 22:16:34 INFO mapreduce.Job:  map 100% reduce 36%
18/12/17 22:16:35 INFO mapreduce.Job:  map 100% reduce 39%
18/12/17 22:16:40 INFO mapreduce.Job:  map 100% reduce 41%
18/12/17 22:16:41 INFO mapreduce.Job:  map 100% reduce 43%
18/12/17 22:16:42 INFO mapreduce.Job:  map 100% reduce 44%
18/12/17 22:16:46 INFO mapreduce.Job:  map 100% reduce 45%
18/12/17 22:16:47 INFO mapreduce.Job:  map 100% reduce 48%
18/12/17 22:16:48 INFO mapreduce.Job:  map 100% reduce 49%
18/12/17 22:16:52 INFO mapreduce.Job:  map 100% reduce 50%
18/12/17 22:16:53 INFO mapreduce.Job:  map 100% reduce 52%
18/12/17 22:16:54 INFO mapreduce.Job:  map 100% reduce 54%
18/12/17 22:16:58 INFO mapreduce.Job:  map 100% reduce 55%
18/12/17 22:16:59 INFO mapreduce.Job:  map 100% reduce 57%
18/12/17 22:17:00 INFO mapreduce.Job:  map 100% reduce 59%
18/12/17 22:17:01 INFO mapreduce.Job:  map 100% reduce 60%
18/12/17 22:17:05 INFO mapreduce.Job:  map 100% reduce 61%
18/12/17 22:17:06 INFO mapreduce.Job:  map 100% reduce 63%
18/12/17 22:17:07 INFO mapreduce.Job:  map 100% reduce 65%
18/12/17 22:17:11 INFO mapreduce.Job:  map 100% reduce 66%
18/12/17 22:17:12 INFO mapreduce.Job:  map 100% reduce 67%
18/12/17 22:17:13 INFO mapreduce.Job:  map 100% reduce 69%
18/12/17 22:17:14 INFO mapreduce.Job:  map 100% reduce 70%
18/12/17 22:17:18 INFO mapreduce.Job:  map 100% reduce 71%
18/12/17 22:17:19 INFO mapreduce.Job:  map 100% reduce 73%
18/12/17 22:17:20 INFO mapreduce.Job:  map 100% reduce 74%
18/12/17 22:17:23 INFO mapreduce.Job:  map 100% reduce 75%
18/12/17 22:17:24 INFO mapreduce.Job:  map 100% reduce 76%
18/12/17 22:17:25 INFO mapreduce.Job:  map 100% reduce 77%
18/12/17 22:17:26 INFO mapreduce.Job:  map 100% reduce 80%
18/12/17 22:17:27 INFO mapreduce.Job:  map 100% reduce 81%
18/12/17 22:17:30 INFO mapreduce.Job:  map 100% reduce 82%
18/12/17 22:17:31 INFO mapreduce.Job:  map 100% reduce 83%
18/12/17 22:17:32 INFO mapreduce.Job:  map 100% reduce 86%
18/12/17 22:17:34 INFO mapreduce.Job:  map 100% reduce 87%
18/12/17 22:17:37 INFO mapreduce.Job:  map 100% reduce 88%
18/12/17 22:17:38 INFO mapreduce.Job:  map 100% reduce 89%
18/12/17 22:17:39 INFO mapreduce.Job:  map 100% reduce 91%
18/12/17 22:17:40 INFO mapreduce.Job:  map 100% reduce 93%
18/12/17 22:17:43 INFO mapreduce.Job:  map 100% reduce 94%
18/12/17 22:17:44 INFO mapreduce.Job:  map 100% reduce 95%
18/12/17 22:17:45 INFO mapreduce.Job:  map 100% reduce 96%
18/12/17 22:17:46 INFO mapreduce.Job:  map 100% reduce 98%
18/12/17 22:17:47 INFO mapreduce.Job:  map 100% reduce 99%
18/12/17 22:17:50 INFO mapreduce.Job:  map 100% reduce 100%
18/12/17 22:17:54 INFO mapreduce.Job: Job job_1545055483525_0061 completed successfully
18/12/17 22:17:54 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=12856249454
		FILE: Number of bytes written=39046017340
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=57716
		HDFS: Number of bytes written=5249783874
		HDFS: Number of read operations=936
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=312
	Job Counters 
		Launched map tasks=156
		Launched reduce tasks=156
		Other local map tasks=156
		Total time spent by all maps in occupied slots (ms)=80457213
		Total time spent by all reduces in occupied slots (ms)=70185855
		Total time spent by all map tasks (ms)=26819071
		Total time spent by all reduce tasks (ms)=23395285
		Total vcore-milliseconds taken by all map tasks=26819071
		Total vcore-milliseconds taken by all reduce tasks=23395285
		Total megabyte-milliseconds taken by all map tasks=82388186112
		Total megabyte-milliseconds taken by all reduce tasks=71870315520
	Map-Reduce Framework
		Map input records=156
		Map output records=3979882881
		Map output bytes=46278917920
		Map output materialized bytes=26139897870
		Input split bytes=13572
		Combine input records=3979882881
		Combine output records=3955135218
		Reduce input groups=120000000
		Reduce shuffle bytes=26139897870
		Reduce input records=3955135218
		Reduce output records=119999999
		Spilled Records=7910270436
		Shuffled Maps =24336
		Failed Shuffles=0
		Merged Map outputs=24336
		GC time elapsed (ms)=5070003
		CPU time spent (ms)=37234660
		Physical memory (bytes) snapshot=544538042368
		Virtual memory (bytes) snapshot=1012186664960
		Total committed heap usage (bytes)=648312520704
	HiBench.Counters
		BYTES_DATA_GENERATED=8273724369
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=44144
	File Output Format Counters 
		Bytes Written=5249783874
18/12/17 22:17:54 INFO HiBench.HiveData: Finished Running Job: Create rankings
18/12/17 22:17:54 INFO HiBench.HiveData: Creating user visits...
18/12/17 22:17:54 INFO HiBench.HiveData: Running Job: Create uservisits
18/12/17 22:17:54 INFO HiBench.HiveData: Dummy file /Join/temp/dummy as input
18/12/17 22:17:54 INFO HiBench.HiveData: Rankings file /Join/Input/rankings as input
18/12/17 22:17:54 INFO HiBench.HiveData: Ouput file /Join/Input/uservisits
18/12/17 22:17:54 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
18/12/17 22:17:54 INFO mapred.FileInputFormat: Total input paths to process : 156
18/12/17 22:17:54 INFO mapred.FileInputFormat: Total input paths to process : 1
18/12/17 22:17:54 INFO mapreduce.JobSubmitter: number of splits:312
18/12/17 22:17:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1545055483525_0062
18/12/17 22:17:55 INFO impl.YarnClientImpl: Submitted application application_1545055483525_0062
18/12/17 22:17:55 INFO mapreduce.Job: The url to track the job: http://cdh-master-1.private1.cdhvcn.oraclevcn.com:8088/proxy/application_1545055483525_0062/
18/12/17 22:17:55 INFO mapreduce.Job: Running job: job_1545055483525_0062
18/12/17 22:18:02 INFO mapreduce.Job: Job job_1545055483525_0062 running in uber mode : false
18/12/17 22:18:02 INFO mapreduce.Job:  map 0% reduce 0%
18/12/17 22:18:23 INFO mapreduce.Job:  map 1% reduce 0%
18/12/17 22:18:24 INFO mapreduce.Job:  map 4% reduce 0%
18/12/17 22:18:25 INFO mapreduce.Job:  map 26% reduce 0%
18/12/17 22:18:26 INFO mapreduce.Job:  map 53% reduce 0%
18/12/17 22:18:27 INFO mapreduce.Job:  map 56% reduce 0%
18/12/17 22:18:28 INFO mapreduce.Job:  map 65% reduce 0%
18/12/17 22:18:29 INFO mapreduce.Job:  map 72% reduce 0%
18/12/17 22:18:30 INFO mapreduce.Job:  map 74% reduce 0%
18/12/17 22:18:31 INFO mapreduce.Job:  map 78% reduce 0%
18/12/17 22:18:32 INFO mapreduce.Job:  map 83% reduce 0%
18/12/17 22:20:13 INFO mapreduce.Job:  map 84% reduce 0%
18/12/17 22:20:16 INFO mapreduce.Job:  map 85% reduce 0%
18/12/17 22:20:24 INFO mapreduce.Job:  map 86% reduce 0%
18/12/17 22:20:35 INFO mapreduce.Job:  map 87% reduce 0%
18/12/17 22:20:37 INFO mapreduce.Job:  map 88% reduce 0%
18/12/17 22:20:41 INFO mapreduce.Job:  map 89% reduce 0%
18/12/17 22:21:18 INFO mapreduce.Job:  map 90% reduce 0%
18/12/17 22:21:20 INFO mapreduce.Job:  map 91% reduce 0%
18/12/17 22:21:21 INFO mapreduce.Job:  map 92% reduce 0%
18/12/17 22:21:23 INFO mapreduce.Job:  map 93% reduce 0%
18/12/17 22:21:25 INFO mapreduce.Job:  map 94% reduce 0%
18/12/17 22:21:27 INFO mapreduce.Job:  map 95% reduce 0%
18/12/17 22:21:30 INFO mapreduce.Job:  map 96% reduce 0%
18/12/17 22:21:34 INFO mapreduce.Job:  map 97% reduce 0%
18/12/17 22:21:39 INFO mapreduce.Job:  map 98% reduce 0%
18/12/17 22:21:43 INFO mapreduce.Job:  map 98% reduce 3%
18/12/17 22:21:44 INFO mapreduce.Job:  map 98% reduce 8%
18/12/17 22:21:46 INFO mapreduce.Job:  map 99% reduce 8%
18/12/17 22:21:49 INFO mapreduce.Job:  map 100% reduce 14%
18/12/17 22:21:50 INFO mapreduce.Job:  map 100% reduce 24%
18/12/17 22:21:51 INFO mapreduce.Job:  map 100% reduce 26%
18/12/17 22:21:52 INFO mapreduce.Job:  map 100% reduce 27%
18/12/17 22:21:53 INFO mapreduce.Job:  map 100% reduce 28%
18/12/17 22:21:54 INFO mapreduce.Job:  map 100% reduce 29%
18/12/17 22:21:55 INFO mapreduce.Job:  map 100% reduce 30%
18/12/17 22:21:56 INFO mapreduce.Job:  map 100% reduce 32%
18/12/17 22:21:57 INFO mapreduce.Job:  map 100% reduce 34%
18/12/17 22:21:58 INFO mapreduce.Job:  map 100% reduce 35%
18/12/17 22:21:59 INFO mapreduce.Job:  map 100% reduce 37%
18/12/17 22:22:04 INFO mapreduce.Job:  map 100% reduce 38%
18/12/17 22:22:06 INFO mapreduce.Job:  map 100% reduce 40%
18/12/17 22:22:07 INFO mapreduce.Job:  map 100% reduce 41%
18/12/17 22:22:08 INFO mapreduce.Job:  map 100% reduce 42%
18/12/17 22:22:09 INFO mapreduce.Job:  map 100% reduce 43%
18/12/17 22:22:11 INFO mapreduce.Job:  map 100% reduce 44%
18/12/17 22:22:12 INFO mapreduce.Job:  map 100% reduce 45%
18/12/17 22:22:13 INFO mapreduce.Job:  map 100% reduce 47%
18/12/17 22:22:14 INFO mapreduce.Job:  map 100% reduce 48%
18/12/17 22:22:15 INFO mapreduce.Job:  map 100% reduce 50%
18/12/17 22:22:18 INFO mapreduce.Job:  map 100% reduce 52%
18/12/17 22:22:19 INFO mapreduce.Job:  map 100% reduce 53%
18/12/17 22:22:20 INFO mapreduce.Job:  map 100% reduce 54%
18/12/17 22:22:21 INFO mapreduce.Job:  map 100% reduce 56%
18/12/17 22:22:24 INFO mapreduce.Job:  map 100% reduce 57%
18/12/17 22:22:26 INFO mapreduce.Job:  map 100% reduce 58%
18/12/17 22:22:27 INFO mapreduce.Job:  map 100% reduce 60%
18/12/17 22:22:28 INFO mapreduce.Job:  map 100% reduce 61%
18/12/17 22:22:31 INFO mapreduce.Job:  map 100% reduce 62%
18/12/17 22:22:32 INFO mapreduce.Job:  map 100% reduce 63%
18/12/17 22:22:33 INFO mapreduce.Job:  map 100% reduce 64%
18/12/17 22:22:34 INFO mapreduce.Job:  map 100% reduce 65%
18/12/17 22:22:37 INFO mapreduce.Job:  map 100% reduce 66%
18/12/17 22:22:39 INFO mapreduce.Job:  map 100% reduce 67%
18/12/17 22:22:43 INFO mapreduce.Job:  map 100% reduce 68%
18/12/17 22:22:55 INFO mapreduce.Job:  map 100% reduce 69%
18/12/17 22:23:11 INFO mapreduce.Job:  map 100% reduce 70%
18/12/17 22:23:25 INFO mapreduce.Job:  map 100% reduce 71%
18/12/17 22:23:38 INFO mapreduce.Job:  map 100% reduce 72%
18/12/17 22:23:50 INFO mapreduce.Job:  map 100% reduce 73%
18/12/17 22:24:05 INFO mapreduce.Job:  map 100% reduce 74%
18/12/17 22:24:18 INFO mapreduce.Job:  map 100% reduce 75%
18/12/17 22:24:32 INFO mapreduce.Job:  map 100% reduce 76%
18/12/17 22:24:45 INFO mapreduce.Job:  map 100% reduce 77%
18/12/17 22:24:57 INFO mapreduce.Job:  map 100% reduce 78%
18/12/17 22:25:12 INFO mapreduce.Job:  map 100% reduce 79%
18/12/17 22:25:27 INFO mapreduce.Job:  map 100% reduce 80%
18/12/17 22:25:40 INFO mapreduce.Job:  map 100% reduce 81%
18/12/17 22:25:53 INFO mapreduce.Job:  map 100% reduce 82%
18/12/17 22:26:08 INFO mapreduce.Job:  map 100% reduce 83%
18/12/17 22:26:19 INFO mapreduce.Job:  map 100% reduce 84%
18/12/17 22:26:32 INFO mapreduce.Job:  map 100% reduce 85%
18/12/17 22:26:47 INFO mapreduce.Job:  map 100% reduce 86%
18/12/17 22:26:57 INFO mapreduce.Job:  map 100% reduce 87%
18/12/17 22:27:09 INFO mapreduce.Job:  map 100% reduce 88%
18/12/17 22:27:22 INFO mapreduce.Job:  map 100% reduce 89%
18/12/17 22:27:34 INFO mapreduce.Job:  map 100% reduce 90%
18/12/17 22:27:47 INFO mapreduce.Job:  map 100% reduce 91%
18/12/17 22:27:59 INFO mapreduce.Job:  map 100% reduce 92%
18/12/17 22:28:11 INFO mapreduce.Job:  map 100% reduce 93%
18/12/17 22:28:28 INFO mapreduce.Job:  map 100% reduce 94%
18/12/17 22:28:46 INFO mapreduce.Job:  map 100% reduce 95%
18/12/17 22:29:02 INFO mapreduce.Job:  map 100% reduce 96%
18/12/17 22:29:19 INFO mapreduce.Job:  map 100% reduce 97%
18/12/17 22:29:33 INFO mapreduce.Job:  map 100% reduce 98%
18/12/17 22:29:50 INFO mapreduce.Job:  map 100% reduce 99%
18/12/17 22:30:04 INFO mapreduce.Job:  map 100% reduce 100%
18/12/17 22:30:15 INFO mapreduce.Job: Job job_1545055483525_0062 completed successfully
18/12/17 22:30:15 INFO mapreduce.Job: Counters: 51
	File System Counters
		FILE: Number of bytes read=13138995832
		FILE: Number of bytes written=41859673784
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=5249899934
		HDFS: Number of bytes written=933610724722
		HDFS: Number of read operations=1560
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=312
	Job Counters 
		Launched map tasks=312
		Launched reduce tasks=156
		Other local map tasks=156
		Data-local map tasks=156
		Total time spent by all maps in occupied slots (ms)=97970565
		Total time spent by all reduces in occupied slots (ms)=226050300
		Total time spent by all map tasks (ms)=32656855
		Total time spent by all reduce tasks (ms)=75350100
		Total vcore-milliseconds taken by all map tasks=32656855
		Total vcore-milliseconds taken by all reduce tasks=75350100
		Total megabyte-milliseconds taken by all map tasks=100321858560
		Total megabyte-milliseconds taken by all reduce tasks=231475507200
	Map-Reduce Framework
		Map input records=120000155
		Map output records=5119999999
		Map output bytes=57680067326
		Map output materialized bytes=28645309268
		Input split bytes=71916
		Combine input records=5119999999
		Combine output records=4507967278
		Reduce input groups=120000000
		Reduce shuffle bytes=28645309268
		Reduce input records=4507967278
		Reduce output records=4999999947
		Spilled Records=9015934556
		Shuffled Maps =48672
		Failed Shuffles=0
		Merged Map outputs=48672
		GC time elapsed (ms)=2134821
		CPU time spent (ms)=84593130
		Physical memory (bytes) snapshot=817922093056
		Virtual memory (bytes) snapshot=1517203525632
		Total committed heap usage (bytes)=991669780480
	HiBench.Counters
		BYTES_DATA_GENERATED=875492935983
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=933610724722
18/12/17 22:30:15 INFO HiBench.HiveData: Finished Running Job: Create uservisits
18/12/17 22:30:15 INFO HiBench.HiveData: Closing hive data generator...
rm: `/Join/Output': No such file or directory
18/12/17 22:30:20 INFO spark.SparkContext: Running Spark version 1.6.0
18/12/17 22:30:21 INFO spark.SecurityManager: Changing view acls to: hdfs
18/12/17 22:30:21 INFO spark.SecurityManager: Changing modify acls to: hdfs
18/12/17 22:30:21 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); users with modify permissions: Set(hdfs)
18/12/17 22:30:21 INFO util.Utils: Successfully started service 'sparkDriver' on port 36549.
18/12/17 22:30:21 INFO slf4j.Slf4jLogger: Slf4jLogger started
18/12/17 22:30:22 INFO Remoting: Starting remoting
18/12/17 22:30:22 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.0.2:40247]
18/12/17 22:30:22 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.0.2:40247]
18/12/17 22:30:22 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 40247.
18/12/17 22:30:22 INFO spark.SparkEnv: Registering MapOutputTracker
18/12/17 22:30:22 INFO spark.SparkEnv: Registering BlockManagerMaster
18/12/17 22:30:22 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-0cfaf685-b77f-4611-8607-bcfdf2e8f057
18/12/17 22:30:22 INFO storage.MemoryStore: MemoryStore started with capacity 530.3 MB
18/12/17 22:30:22 INFO spark.SparkEnv: Registering OutputCommitCoordinator
18/12/17 22:30:22 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
18/12/17 22:30:22 INFO ui.SparkUI: Started SparkUI at http://10.0.0.2:4040
18/12/17 22:30:22 INFO spark.SparkContext: Added JAR file:/var/lib/hadoop-hdfs/sparkbench-assembly-7.1-SNAPSHOT-dist.jar at spark://10.0.0.2:36549/jars/sparkbench-assembly-7.1-SNAPSHOT-dist.jar with timestamp 1545085822648
18/12/17 22:30:23 INFO yarn.Client: Requesting a new application from cluster with 3 NodeManagers
18/12/17 22:30:23 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
18/12/17 22:30:23 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
18/12/17 22:30:23 INFO yarn.Client: Setting up container launch context for our AM
18/12/17 22:30:23 INFO yarn.Client: Setting up the launch environment for our AM container
18/12/17 22:30:23 INFO yarn.Client: Preparing resources for our AM container
18/12/17 22:30:23 INFO yarn.Client: Uploading resource file:/tmp/spark-81e34d53-9be4-4fb9-ae38-e86b6eccf9ee/__spark_conf__4949445140530909959.zip -> hdfs://nameservice1/user/hdfs/.sparkStaging/application_1545055483525_0063/__spark_conf__4949445140530909959.zip
18/12/17 22:30:24 INFO spark.SecurityManager: Changing view acls to: hdfs
18/12/17 22:30:24 INFO spark.SecurityManager: Changing modify acls to: hdfs
18/12/17 22:30:24 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); users with modify permissions: Set(hdfs)
18/12/17 22:30:24 INFO yarn.Client: Submitting application 63 to ResourceManager
18/12/17 22:30:24 INFO impl.YarnClientImpl: Submitted application application_1545055483525_0063
18/12/17 22:30:25 INFO yarn.Client: Application report for application_1545055483525_0063 (state: ACCEPTED)
18/12/17 22:30:25 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: root.users.hdfs
	 start time: 1545085824186
	 final status: UNDEFINED
	 tracking URL: http://cdh-master-1.private1.cdhvcn.oraclevcn.com:8088/proxy/application_1545055483525_0063/
	 user: hdfs
18/12/17 22:30:26 INFO yarn.Client: Application report for application_1545055483525_0063 (state: ACCEPTED)
18/12/17 22:30:27 INFO yarn.Client: Application report for application_1545055483525_0063 (state: ACCEPTED)
18/12/17 22:30:28 INFO yarn.Client: Application report for application_1545055483525_0063 (state: ACCEPTED)
18/12/17 22:30:28 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(null)
18/12/17 22:30:28 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> cdh-master-1.private1.cdhvcn.oraclevcn.com,cdh-master-2.private2.cdhvcn.oraclevcn.com, PROXY_URI_BASES -> http://cdh-master-1.private1.cdhvcn.oraclevcn.com:8088/proxy/application_1545055483525_0063,http://cdh-master-2.private2.cdhvcn.oraclevcn.com:8088/proxy/application_1545055483525_0063), /proxy/application_1545055483525_0063
18/12/17 22:30:28 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
18/12/17 22:30:29 INFO yarn.Client: Application report for application_1545055483525_0063 (state: RUNNING)
18/12/17 22:30:29 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 10.0.5.2
	 ApplicationMaster RPC port: 0
	 queue: root.users.hdfs
	 start time: 1545085824186
	 final status: UNDEFINED
	 tracking URL: http://cdh-master-1.private1.cdhvcn.oraclevcn.com:8088/proxy/application_1545055483525_0063/
	 user: hdfs
18/12/17 22:30:29 INFO cluster.YarnClientSchedulerBackend: Application application_1545055483525_0063 has started running.
18/12/17 22:30:29 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37095.
18/12/17 22:30:29 INFO netty.NettyBlockTransferService: Server created on 37095
18/12/17 22:30:29 INFO storage.BlockManager: external shuffle service port = 7337
18/12/17 22:30:29 INFO storage.BlockManagerMaster: Trying to register BlockManager
18/12/17 22:30:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.0.2:37095 with 530.3 MB RAM, BlockManagerId(driver, 10.0.0.2, 37095)
18/12/17 22:30:29 INFO storage.BlockManagerMaster: Registered BlockManager
18/12/17 22:30:29 INFO scheduler.EventLoggingListener: Logging events to hdfs://nameservice1/user/spark/applicationHistory/application_1545055483525_0063
18/12/17 22:30:29 INFO spark.SparkContext: Registered listener com.cloudera.spark.lineage.ClouderaNavigatorListener
18/12/17 22:30:29 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
18/12/17 22:30:30 INFO hive.HiveContext: Initializing execution hive, version 1.1.0
18/12/17 22:30:30 INFO client.ClientWrapper: Inspected Hadoop version: 2.6.0-cdh5.16.1
18/12/17 22:30:30 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0-cdh5.16.1
18/12/17 22:30:30 INFO session.SessionState: Created HDFS directory: file:/tmp/spark-a01ef45f-0fa4-4b9b-b28d-361ed21f55ba/scratch/hdfs
18/12/17 22:30:30 INFO session.SessionState: Created local directory: /tmp/e2426cf2-8977-41c8-9b73-47298f7292f3_resources
18/12/17 22:30:30 INFO session.SessionState: Created HDFS directory: file:/tmp/spark-a01ef45f-0fa4-4b9b-b28d-361ed21f55ba/scratch/hdfs/e2426cf2-8977-41c8-9b73-47298f7292f3
18/12/17 22:30:30 INFO session.SessionState: Created local directory: /tmp/hdfs/e2426cf2-8977-41c8-9b73-47298f7292f3
18/12/17 22:30:30 INFO session.SessionState: Created HDFS directory: file:/tmp/spark-a01ef45f-0fa4-4b9b-b28d-361ed21f55ba/scratch/hdfs/e2426cf2-8977-41c8-9b73-47298f7292f3/_tmp_space.db
18/12/17 22:30:30 INFO session.SessionState: No Tez session required at this point. hive.execution.engine=mr.
18/12/17 22:30:31 INFO hive.metastore: Trying to connect to metastore with URI thrift://cdh-utility-1.public1.cdhvcn.oraclevcn.com:9083
18/12/17 22:30:31 INFO hive.metastore: Opened a connection to metastore, current connections: 1
18/12/17 22:30:31 INFO hive.metastore: Connected to metastore.
18/12/17 22:30:31 INFO hive.HiveContext: default warehouse location is /user/hive/warehouse
18/12/17 22:30:31 INFO hive.HiveContext: Initializing metastore client version 1.1.0 using Spark classes.
18/12/17 22:30:31 INFO client.ClientWrapper: Inspected Hadoop version: 2.6.0-cdh5.16.1
18/12/17 22:30:31 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0-cdh5.16.1
18/12/17 22:30:32 INFO session.SessionState: Created local directory: /tmp/6590118c-7cc4-424f-a604-bf16c141021f_resources
18/12/17 22:30:32 INFO session.SessionState: Created HDFS directory: /tmp/hive/hdfs/6590118c-7cc4-424f-a604-bf16c141021f
18/12/17 22:30:32 INFO session.SessionState: Created local directory: /tmp/hdfs/6590118c-7cc4-424f-a604-bf16c141021f
18/12/17 22:30:32 INFO session.SessionState: Created HDFS directory: /tmp/hive/hdfs/6590118c-7cc4-424f-a604-bf16c141021f/_tmp_space.db
18/12/17 22:30:32 INFO session.SessionState: No Tez session required at this point. hive.execution.engine=mr.
18/12/17 22:30:32 INFO hive.metastore: Trying to connect to metastore with URI thrift://cdh-utility-1.public1.cdhvcn.oraclevcn.com:9083
18/12/17 22:30:32 INFO hive.metastore: Opened a connection to metastore, current connections: 1
18/12/17 22:30:32 INFO hive.metastore: Connected to metastore.
18/12/17 22:30:33 INFO log.PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:33 INFO log.PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:33 INFO log.PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:33 INFO ql.Driver: Compiling command(queryId=): USE DEFAULT
18/12/17 22:30:33 INFO log.PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:34 INFO log.PerfLogger: </PERFLOG method=parse start=1545085833707 end=1545085834031 duration=324 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:34 INFO log.PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:34 INFO ql.Driver: Semantic Analysis Completed
18/12/17 22:30:34 INFO log.PerfLogger: </PERFLOG method=semanticAnalyze start=1545085834034 end=1545085834086 duration=52 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:34 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
18/12/17 22:30:34 INFO log.PerfLogger: </PERFLOG method=compile start=1545085833673 end=1545085834093 duration=420 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:34 INFO log.PerfLogger: </PERFLOG method=compile start=1545085833673 end=1545085834093 duration=420 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:34 INFO ql.Driver: Completed compiling command(queryId=); Time taken: 0.42 seconds
18/12/17 22:30:34 INFO log.PerfLogger: <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:34 INFO lockmgr.DummyTxnManager: Creating lock manager of type org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager
18/12/17 22:30:34 INFO imps.CuratorFrameworkImpl: Starting
18/12/17 22:30:34 INFO zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.5-1392090, built on 09/30/2012 17:52 GMT
18/12/17 22:30:34 INFO zookeeper.ZooKeeper: Client environment:host.name=cdh-utility-1.public1.cdhvcn.oraclevcn.com
18/12/17 22:30:34 INFO zookeeper.ZooKeeper: Client environment:java.version=1.7.0_67
18/12/17 22:30:34 INFO zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation
18/12/17 22:30:34 INFO zookeeper.ZooKeeper: Client environment:java.home=/usr/java/jdk1.7.0_67-cloudera/jre
18/12/17 22:30:34 INFO zookeeper.ZooKeeper: Client environment:java.class.path=/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/lib/spark/conf/:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/lib/spark/lib/spark-assembly-1.6.0-cdh5.16.1-hadoop2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/lib/spark/conf/yarn-conf/:/etc/hive/conf/:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/ST4-4.0.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/accumulo-core-1.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/accumulo-fate-1.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/accumulo-start-1.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/accumulo-trace-1.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/activation-1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/ant-1.9.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/ant-launcher-1.9.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/antlr-2.7.7.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/antlr-runtime-3.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/aopalliance-1.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/apache-log4j-extras-1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/apache-log4j-extras-1.2.17.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/apacheds-i18n-2.0.0-M15.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/api-asn1-api-1.0.0-M20.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/api-util-1.0.0-M20.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/asm-3.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/asm-commons-3.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/asm-tree-3.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/async-1.4.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/asynchbase-1.7.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-compiler-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-ipc-1.7.6-cdh5.16.1-tests.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-ipc-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-mapred-1.7.6-cdh5.16.1-hadoop2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-maven-plugin-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-protobuf-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-service-archetype-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-thrift-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/aws-java-sdk-bundle-1.11.134.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/azure-data-lake-store-sdk-2.2.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/bonecp-0.8.0.RELEASE.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/calcite-avatica-1.0.0-incubating.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/calcite-core-1.0.0-incubating.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/calcite-linq4j-1.0.0-incubating.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-beanutils-1.9.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-beanutils-core-1.8.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-cli-1.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-codec-1.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-codec-1.8.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-collections-3.2.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-compiler-2.7.6.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-compress-1.4.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-configuration-1.6.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-daemon-1.0.13.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-dbcp-1.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-digester-1.8.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-el-1.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-httpclient-3.0.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-httpclient-3.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-io-2.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-jexl-2.1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-lang-2.6.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-lang3-3.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-logging-1.1.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-math-2.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-math3-3.1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-net-3.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-pool-1.5.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-vfs2-2.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/curator-client-2.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/curator-client-2.7.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/curator-framework-2.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/curator-framework-2.7.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/curator-recipes-2.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/curator-recipes-2.7.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/datanucleus-api-jdo-3.2.6.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/datanucleus-core-3.2.10.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/datanucleus-rdbms-3.2.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/derby-10.11.1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/eigenbase-properties-1.1.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/fastutil-6.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/findbugs-annotations-1.3.9-1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-avro-source-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-dataset-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-file-channel-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-hdfs-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-hive-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-irc-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-jdbc-channel-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-jms-source-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-kafka-channel-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-kafka-source-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-auth-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-config-filter-api-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-configuration-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-core-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-elasticsearch-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-embedded-agent-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-environment-variable-config-filter-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-external-process-config-filter-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-hadoop-credential-store-config-filter-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-hbase-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-kafka-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-log4jappender-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-morphline-solr-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-node-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-sdk-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-scribe-source-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-spillable-memory-channel-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-taildir-source-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-thrift-source-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-tools-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-twitter-source-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/geronimo-annotation_1.0_spec-1.1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/geronimo-jaspic_1.0_spec-1.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/geronimo-jta_1.1_spec-1.1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/groovy-all-2.4.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/gson-2.2.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/guava-11.0.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/guava-11.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/guava-14.0.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/guice-3.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/guice-servlet-3.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-annotations-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-ant-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-archive-logs-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-archives-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-auth-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-aws-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-azure-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-azure-datalake-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-common-2.6.0-cdh5.16.1-tests.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-common-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-datajoin-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-distcp-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-extras-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-gridmix-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-hdfs-2.6.0-cdh5.16.1-tests.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-hdfs-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-hdfs-nfs-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-app-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-common-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-core-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-hs-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-hs-plugins-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.16.1-tests.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-nativetask-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-examples-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-nfs-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-openstack-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-rumen-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-sls-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-streaming-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-api-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-applications-distributedshell-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-client-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-common-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-registry-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-server-applicationhistoryservice-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-server-common-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-server-nodemanager-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-server-resourcemanager-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-server-tests-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-server-web-proxy-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hamcrest-core-1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hamcrest-core-1.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hbase-annotations-1.2.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hbase-client-1.2.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hbase-common-1.2.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hbase-hadoop-compat-1.2.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hbase-hadoop2-compat-1.2.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hbase-protocol-1.2.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hbase-server-1.2.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/high-scale-lib-1.1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-accumulo-handler-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-ant-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-beeline-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-classification-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-cli-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-common-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-contrib-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-exec-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-hbase-handler-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-hwi-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-jdbc-1.1.0-cdh5.16.1-standalone.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-jdbc-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-metastore-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-serde-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-service-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-shims-0.23-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-shims-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-shims-common-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-shims-scheduler-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-testutils-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/htrace-core-3.2.0-incubating.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/htrace-core4-4.0.1-incubating.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/httpclient-4.2.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/httpcore-4.2.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hue-plugins-3.9.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/irclib-1.10.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/ivy-2.0.0-rc2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jackson-annotations-2.2.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jackson-core-2.2.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jackson-core-asl-1.8.10.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jackson-databind-2.2.3-cloudera.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jackson-jaxrs-1.8.10.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jackson-mapper-asl-1.8.10-cloudera.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jackson-xc-1.8.10.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jamon-runtime-2.3.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/janino-2.7.6.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jasper-compiler-5.5.23.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jasper-runtime-5.5.23.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/java-xmlbuilder-0.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/javax.inject-1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jaxb-api-2.2.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jaxb-impl-2.2.3-1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jcommander-1.32.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jdo-api-3.0.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jersey-client-1.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jersey-core-1.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jersey-guice-1.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jersey-json-1.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jersey-server-1.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jets3t-0.6.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jets3t-0.9.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jettison-1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jetty-6.1.26.cloudera.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jetty-all-7.6.0.v20120127.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jetty-all-server-7.6.0.v20120127.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jetty-util-6.1.26.cloudera.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jline-2.11.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jline-2.12.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/joda-time-1.6.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/joda-time-2.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jopt-simple-5.0.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jpam-1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jsch-0.1.42.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jsp-api-2.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jsr305-1.3.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jsr305-3.0.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jta-1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/kafka-clients-0.10.2-kafka-2.2.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/kafka_2.10-0.10.2-kafka-2.2.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/kite-data-core-1.0.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/kite-data-hbase-1.0.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/kite-data-hive-1.0.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/kite-hadoop-compatibility-1.0.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/leveldbjni-all-1.8.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/libfb303-0.9.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/libthrift-0.9.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/log4j-1.2.16.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/log4j-1.2.17.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/logredactor-1.0.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/lz4-1.3.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/mail-1.4.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/mapdb-0.9.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/maven-scm-api-1.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/maven-scm-provider-svn-commons-1.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/maven-scm-provider-svnexe-1.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/metrics-core-2.2.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/metrics-core-3.0.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/metrics-json-3.0.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/metrics-jvm-3.0.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/microsoft-windowsazure-storage-sdk-0.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/mina-core-2.0.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/mockito-all-1.8.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/netty-3.10.5.Final.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/netty-3.9.4.Final.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/netty-all-4.0.23.Final.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/okhttp-2.4.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/okio-1.4.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/opencsv-2.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/oro-2.0.8.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/paranamer-2.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-avro-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-cascading-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-column-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-common-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-encoding-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-format-2.1.0-cdh5.16.1-javadoc.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-format-2.1.0-cdh5.16.1-sources.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-format-2.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-generator-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-hadoop-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-hadoop-bundle-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-jackson-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-pig-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-pig-bundle-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-protobuf-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-scala_2.10-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-scrooge_2.10-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-test-hadoop2-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-thrift-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-tools-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/plexus-utils-1.5.6.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/protobuf-java-2.5.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/regexp-1.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/scala-library-2.10.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/serializer-2.7.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/servlet-api-2.5-20110124.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/servlet-api-2.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/slf4j-api-1.7.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/slf4j-log4j12-1.7.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/snappy-java-1.0.4.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/spark-1.6.0-cdh5.16.1-yarn-shuffle.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/spark-streaming-flume-sink_2.10-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/stax-api-1.0-2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/stax-api-1.0.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/stringtemplate-3.2.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/super-csv-2.2.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/tempus-fugit-1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/trevni-avro-1.7.6-cdh5.16.1-hadoop2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/trevni-avro-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/trevni-core-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/twitter4j-core-3.0.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/twitter4j-media-support-3.0.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/twitter4j-stream-3.0.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/unused-1.0.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/velocity-1.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/velocity-1.7.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/xalan-2.7.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/xercesImpl-2.9.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/xml-apis-1.3.04.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/xmlenc-0.52.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/xz-1.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/zkclient-0.10.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/zookeeper-3.4.5-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/lib/hadoop/LICENSE.txt:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/lib/hadoop/NOTICE.txt
18/12/17 22:30:34 INFO zookeeper.ZooKeeper: Client environment:java.library.path=:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/bin/../lib/hadoop/lib/native:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/lib/hadoop/lib/native:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
18/12/17 22:30:34 INFO zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
18/12/17 22:30:34 INFO zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
18/12/17 22:30:34 INFO zookeeper.ZooKeeper: Client environment:os.name=Linux
18/12/17 22:30:34 INFO zookeeper.ZooKeeper: Client environment:os.arch=amd64
18/12/17 22:30:34 INFO zookeeper.ZooKeeper: Client environment:os.version=3.10.0-862.3.3.el7.x86_64
18/12/17 22:30:34 INFO zookeeper.ZooKeeper: Client environment:user.name=hdfs
18/12/17 22:30:34 INFO zookeeper.ZooKeeper: Client environment:user.home=/var/lib/hadoop-hdfs
18/12/17 22:30:34 INFO zookeeper.ZooKeeper: Client environment:user.dir=/var/lib/hadoop-hdfs/benchmark-baremetal/src
18/12/17 22:30:34 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=cdh-utility-1.public1.cdhvcn.oraclevcn.com:2181,cdh-master-1.private1.cdhvcn.oraclevcn.com:2181,cdh-master-2.private2.cdhvcn.oraclevcn.com:2181 sessionTimeout=1200000 watcher=org.apache.curator.ConnectionState@32fb8afb
18/12/17 22:30:34 INFO zookeeper.ClientCnxn: Opening socket connection to server cdh-master-1.private1.cdhvcn.oraclevcn.com/10.0.3.2:2181. Will not attempt to authenticate using SASL (unknown error)
18/12/17 22:30:34 INFO zookeeper.ClientCnxn: Socket connection established to cdh-master-1.private1.cdhvcn.oraclevcn.com/10.0.3.2:2181, initiating session
18/12/17 22:30:34 INFO zookeeper.ClientCnxn: Session establishment complete on server cdh-master-1.private1.cdhvcn.oraclevcn.com/10.0.3.2:2181, sessionid = 0x267bc7ae61e02a1, negotiated timeout = 40000
18/12/17 22:30:34 INFO state.ConnectionStateManager: State change: CONNECTED
18/12/17 22:30:35 INFO log.PerfLogger: </PERFLOG method=acquireReadWriteLocks start=1545085834097 end=1545085835183 duration=1086 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO log.PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO ql.Driver: Executing command(queryId=): USE DEFAULT
18/12/17 22:30:35 INFO log.PerfLogger: </PERFLOG method=TimeToSubmit start=1545085833673 end=1545085835187 duration=1514 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO log.PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO ql.Driver: Starting task [Stage-0:DDL] in serial mode
18/12/17 22:30:35 INFO log.PerfLogger: </PERFLOG method=runTasks start=1545085835188 end=1545085835198 duration=10 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO log.PerfLogger: </PERFLOG method=Driver.execute start=1545085835183 end=1545085835198 duration=15 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO ql.Driver: Completed executing command(queryId=); Time taken: 0.015 seconds
18/12/17 22:30:35 INFO ql.Driver: OK
18/12/17 22:30:35 INFO log.PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO log.PerfLogger: </PERFLOG method=releaseLocks start=1545085835198 end=1545085835199 duration=1 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO log.PerfLogger: </PERFLOG method=Driver.run start=1545085833672 end=1545085835199 duration=1527 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO log.PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO log.PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO log.PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO ql.Driver: Compiling command(queryId=): DROP TABLE IF EXISTS rankings
18/12/17 22:30:35 INFO log.PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO log.PerfLogger: </PERFLOG method=parse start=1545085835379 end=1545085835381 duration=2 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO log.PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO ql.Driver: Semantic Analysis Completed
18/12/17 22:30:35 INFO log.PerfLogger: </PERFLOG method=semanticAnalyze start=1545085835381 end=1545085835422 duration=41 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
18/12/17 22:30:35 INFO log.PerfLogger: </PERFLOG method=compile start=1545085835379 end=1545085835422 duration=43 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO log.PerfLogger: </PERFLOG method=compile start=1545085835379 end=1545085835422 duration=43 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO ql.Driver: Completed compiling command(queryId=); Time taken: 0.043 seconds
18/12/17 22:30:35 INFO log.PerfLogger: <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO log.PerfLogger: </PERFLOG method=acquireReadWriteLocks start=1545085835423 end=1545085835423 duration=0 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO log.PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO ql.Driver: Executing command(queryId=): USE DEFAULT
18/12/17 22:30:35 INFO log.PerfLogger: </PERFLOG method=TimeToSubmit start=1545085835379 end=1545085835423 duration=44 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO log.PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO ql.Driver: Starting task [Stage-0:DDL] in serial mode
18/12/17 22:30:35 ERROR metadata.Hive: Table rankings not found: DEFAULT.rankings table not found
18/12/17 22:30:35 INFO log.PerfLogger: </PERFLOG method=runTasks start=1545085835423 end=1545085835436 duration=13 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO log.PerfLogger: </PERFLOG method=Driver.execute start=1545085835423 end=1545085835437 duration=14 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO ql.Driver: Completed executing command(queryId=); Time taken: 0.014 seconds
18/12/17 22:30:35 INFO ql.Driver: OK
18/12/17 22:30:35 INFO log.PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO log.PerfLogger: </PERFLOG method=releaseLocks start=1545085835437 end=1545085835437 duration=0 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:30:35 INFO log.PerfLogger: </PERFLOG method=Driver.run start=1545085835379 end=1545085835437 duration=58 from=org.apache.hadoop.hive.ql.Driver>
Exception in thread "main" org.apache.spark.sql.AnalysisException: missing EOF at 'NCEFILE' near 'SEQUE'; line 2 pos 0
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:318)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:295)
	at org.apache.spark.sql.hive.HiveQLDialect$$anonfun$parse$1.apply(HiveContext.scala:66)
	at org.apache.spark.sql.hive.HiveQLDialect$$anonfun$parse$1.apply(HiveContext.scala:66)
	at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1.apply(ClientWrapper.scala:293)
	at org.apache.spark.sql.hive.client.ClientWrapper.liftedTree1$1(ClientWrapper.scala:239)
	at org.apache.spark.sql.hive.client.ClientWrapper.retryLocked(ClientWrapper.scala:238)
	at org.apache.spark.sql.hive.client.ClientWrapper.withHiveState(ClientWrapper.scala:281)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:65)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:211)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:211)
	at org.apache.spark.sql.execution.SparkSQLParser$$anonfun$org$apache$spark$sql$execution$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at org.apache.spark.sql.execution.SparkSQLParser$$anonfun$org$apache$spark$sql$execution$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:113)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:208)
	at org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:208)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:43)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:231)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:334)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:829)
	at com.intel.hibench.sparkbench.sql.ScalaSparkSQLBench$$anonfun$main$1.apply(ScalaSparkSQLBench.scala:43)
	at com.intel.hibench.sparkbench.sql.ScalaSparkSQLBench$$anonfun$main$1.apply(ScalaSparkSQLBench.scala:41)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at com.intel.hibench.sparkbench.sql.ScalaSparkSQLBench$.main(ScalaSparkSQLBench.scala:41)
	at com.intel.hibench.sparkbench.sql.ScalaSparkSQLBench.main(ScalaSparkSQLBench.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:730)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
18/12/17 22:30:35 INFO spark.SparkContext: Invoking stop() from shutdown hook
18/12/17 22:30:35 INFO zookeeper.ZooKeeper: Session: 0x267bc7ae61e02a1 closed
18/12/17 22:30:35 INFO zookeeper.ClientCnxn: EventThread shut down
18/12/17 22:30:35 INFO CuratorFrameworkSingleton: Closing ZooKeeper client.
18/12/17 22:30:35 INFO ui.SparkUI: Stopped Spark web UI at http://10.0.0.2:4040
18/12/17 22:30:35 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread
18/12/17 22:30:35 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors
18/12/17 22:30:35 INFO cluster.YarnClientSchedulerBackend: Asking each executor to shut down
18/12/17 22:30:35 INFO cluster.YarnClientSchedulerBackend: Stopped
18/12/17 22:30:35 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/12/17 22:30:35 INFO storage.MemoryStore: MemoryStore cleared
18/12/17 22:30:35 INFO storage.BlockManager: BlockManager stopped
18/12/17 22:30:35 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
18/12/17 22:30:35 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/12/17 22:30:35 INFO spark.SparkContext: Successfully stopped SparkContext
18/12/17 22:30:35 INFO util.ShutdownHookManager: Shutdown hook called
18/12/17 22:30:35 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-a01ef45f-0fa4-4b9b-b28d-361ed21f55ba
18/12/17 22:30:35 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-81e34d53-9be4-4fb9-ae38-e86b6eccf9ee
18/12/17 22:30:35 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
18/12/17 22:30:35 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
