rm: `/Scan/Input': No such file or directory
18/12/17 22:30:43 INFO HiBench.HiveData: Generating hive data files...
18/12/17 22:30:43 INFO HiBench.HiveData: Initializing hive date generator...
18/12/17 22:30:45 INFO HiBench.Dummy: Creating dummy file /Scan/temp/dummy with 156 slots...
18/12/17 22:30:45 INFO HiBench.HiveData: Creating table rankings...
18/12/17 22:30:45 INFO Configuration.deprecation: mapred.reduce.slowstart.completed.maps is deprecated. Instead, use mapreduce.job.reduce.slowstart.completedmaps
18/12/17 22:30:45 INFO HiBench.HiveData: Running Job: Create rankings
18/12/17 22:30:45 INFO HiBench.HiveData: Pages file /Scan/temp/dummy as input
18/12/17 22:30:45 INFO HiBench.HiveData: Rankings file /Scan/Input/rankings as output
18/12/17 22:30:46 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
18/12/17 22:30:46 INFO mapred.FileInputFormat: Total input paths to process : 1
18/12/17 22:30:46 INFO mapreduce.JobSubmitter: number of splits:156
18/12/17 22:30:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1545055483525_0064
18/12/17 22:30:47 INFO impl.YarnClientImpl: Submitted application application_1545055483525_0064
18/12/17 22:30:47 INFO mapreduce.Job: The url to track the job: http://cdh-master-1.private1.cdhvcn.oraclevcn.com:8088/proxy/application_1545055483525_0064/
18/12/17 22:30:47 INFO mapreduce.Job: Running job: job_1545055483525_0064
18/12/17 22:30:54 INFO mapreduce.Job: Job job_1545055483525_0064 running in uber mode : false
18/12/17 22:30:54 INFO mapreduce.Job:  map 0% reduce 0%
18/12/17 22:31:11 INFO mapreduce.Job:  map 3% reduce 0%
18/12/17 22:31:12 INFO mapreduce.Job:  map 10% reduce 0%
18/12/17 22:31:13 INFO mapreduce.Job:  map 24% reduce 0%
18/12/17 22:31:14 INFO mapreduce.Job:  map 33% reduce 0%
18/12/17 22:31:16 INFO mapreduce.Job:  map 36% reduce 0%
18/12/17 22:31:17 INFO mapreduce.Job:  map 43% reduce 0%
18/12/17 22:31:18 INFO mapreduce.Job:  map 45% reduce 0%
18/12/17 22:31:19 INFO mapreduce.Job:  map 66% reduce 0%
18/12/17 22:31:20 INFO mapreduce.Job:  map 80% reduce 0%
18/12/17 22:31:21 INFO mapreduce.Job:  map 81% reduce 0%
18/12/17 22:31:22 INFO mapreduce.Job:  map 84% reduce 0%
18/12/17 22:31:23 INFO mapreduce.Job:  map 87% reduce 0%
18/12/17 22:31:24 INFO mapreduce.Job:  map 90% reduce 0%
18/12/17 22:31:25 INFO mapreduce.Job:  map 94% reduce 0%
18/12/17 22:31:26 INFO mapreduce.Job:  map 98% reduce 0%
18/12/17 22:31:27 INFO mapreduce.Job:  map 100% reduce 0%
18/12/17 22:31:32 INFO mapreduce.Job:  map 100% reduce 33%
18/12/17 22:31:36 INFO mapreduce.Job:  map 100% reduce 47%
18/12/17 22:31:37 INFO mapreduce.Job:  map 100% reduce 99%
18/12/17 22:31:38 INFO mapreduce.Job:  map 100% reduce 100%
18/12/17 22:31:40 INFO mapreduce.Job: Job job_1545055483525_0064 completed successfully
18/12/17 22:31:40 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=1071402042
		FILE: Number of bytes written=3299006459
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=57716
		HDFS: Number of bytes written=437603691
		HDFS: Number of read operations=936
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=312
	Job Counters 
		Launched map tasks=156
		Launched reduce tasks=156
		Other local map tasks=156
		Total time spent by all maps in occupied slots (ms)=11086824
		Total time spent by all reduces in occupied slots (ms)=8317470
		Total time spent by all map tasks (ms)=3695608
		Total time spent by all reduce tasks (ms)=2772490
		Total vcore-milliseconds taken by all map tasks=3695608
		Total vcore-milliseconds taken by all reduce tasks=2772490
		Total megabyte-milliseconds taken by all map tasks=11352907776
		Total megabyte-milliseconds taken by all reduce tasks=8517089280
	Map-Reduce Framework
		Map input records=156
		Map output records=331705180
		Map output bytes=3857087670
		Map output materialized bytes=2177735337
		Input split bytes=13572
		Combine input records=331705180
		Combine output records=329643265
		Reduce input groups=10000000
		Reduce shuffle bytes=2177735337
		Reduce input records=329643265
		Reduce output records=10000000
		Spilled Records=659286530
		Shuffled Maps =24336
		Failed Shuffles=0
		Merged Map outputs=24336
		GC time elapsed (ms)=80865
		CPU time spent (ms)=3900930
		Physical memory (bytes) snapshot=338610327552
		Virtual memory (bytes) snapshot=1012770586624
		Total committed heap usage (bytes)=652348489728
	HiBench.Counters
		BYTES_DATA_GENERATED=689506691
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=44144
	File Output Format Counters 
		Bytes Written=437603691
18/12/17 22:31:40 INFO HiBench.HiveData: Finished Running Job: Create rankings
18/12/17 22:31:40 INFO HiBench.HiveData: Creating user visits...
18/12/17 22:31:40 INFO HiBench.HiveData: Running Job: Create uservisits
18/12/17 22:31:40 INFO HiBench.HiveData: Dummy file /Scan/temp/dummy as input
18/12/17 22:31:40 INFO HiBench.HiveData: Rankings file /Scan/Input/rankings as input
18/12/17 22:31:40 INFO HiBench.HiveData: Ouput file /Scan/Input/uservisits
18/12/17 22:31:40 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
18/12/17 22:31:41 INFO mapred.FileInputFormat: Total input paths to process : 156
18/12/17 22:31:41 INFO mapred.FileInputFormat: Total input paths to process : 1
18/12/17 22:31:41 INFO mapreduce.JobSubmitter: number of splits:312
18/12/17 22:31:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1545055483525_0065
18/12/17 22:31:41 INFO impl.YarnClientImpl: Submitted application application_1545055483525_0065
18/12/17 22:31:41 INFO mapreduce.Job: The url to track the job: http://cdh-master-1.private1.cdhvcn.oraclevcn.com:8088/proxy/application_1545055483525_0065/
18/12/17 22:31:41 INFO mapreduce.Job: Running job: job_1545055483525_0065
18/12/17 22:31:48 INFO mapreduce.Job: Job job_1545055483525_0065 running in uber mode : false
18/12/17 22:31:48 INFO mapreduce.Job:  map 0% reduce 0%
18/12/17 22:32:05 INFO mapreduce.Job:  map 2% reduce 0%
18/12/17 22:32:06 INFO mapreduce.Job:  map 4% reduce 0%
18/12/17 22:32:07 INFO mapreduce.Job:  map 13% reduce 0%
18/12/17 22:32:08 INFO mapreduce.Job:  map 22% reduce 0%
18/12/17 22:32:09 INFO mapreduce.Job:  map 27% reduce 0%
18/12/17 22:32:10 INFO mapreduce.Job:  map 33% reduce 0%
18/12/17 22:32:11 INFO mapreduce.Job:  map 44% reduce 0%
18/12/17 22:32:12 INFO mapreduce.Job:  map 56% reduce 0%
18/12/17 22:32:13 INFO mapreduce.Job:  map 83% reduce 0%
18/12/17 22:32:48 INFO mapreduce.Job:  map 84% reduce 0%
18/12/17 22:32:49 INFO mapreduce.Job:  map 85% reduce 0%
18/12/17 22:32:50 INFO mapreduce.Job:  map 86% reduce 0%
18/12/17 22:32:52 INFO mapreduce.Job:  map 87% reduce 0%
18/12/17 22:32:53 INFO mapreduce.Job:  map 88% reduce 0%
18/12/17 22:32:56 INFO mapreduce.Job:  map 89% reduce 0%
18/12/17 22:33:08 INFO mapreduce.Job:  map 90% reduce 0%
18/12/17 22:33:09 INFO mapreduce.Job:  map 92% reduce 0%
18/12/17 22:33:10 INFO mapreduce.Job:  map 94% reduce 0%
18/12/17 22:33:11 INFO mapreduce.Job:  map 95% reduce 0%
18/12/17 22:33:12 INFO mapreduce.Job:  map 96% reduce 0%
18/12/17 22:33:13 INFO mapreduce.Job:  map 98% reduce 0%
18/12/17 22:33:15 INFO mapreduce.Job:  map 99% reduce 0%
18/12/17 22:33:20 INFO mapreduce.Job:  map 100% reduce 0%
18/12/17 22:33:28 INFO mapreduce.Job:  map 100% reduce 4%
18/12/17 22:33:34 INFO mapreduce.Job:  map 100% reduce 5%
18/12/17 22:33:35 INFO mapreduce.Job:  map 100% reduce 7%
18/12/17 22:33:36 INFO mapreduce.Job:  map 100% reduce 16%
18/12/17 22:33:37 INFO mapreduce.Job:  map 100% reduce 21%
18/12/17 22:33:38 INFO mapreduce.Job:  map 100% reduce 30%
18/12/17 22:33:39 INFO mapreduce.Job:  map 100% reduce 32%
18/12/17 22:33:41 INFO mapreduce.Job:  map 100% reduce 33%
18/12/17 22:33:42 INFO mapreduce.Job:  map 100% reduce 35%
18/12/17 22:33:43 INFO mapreduce.Job:  map 100% reduce 36%
18/12/17 22:33:44 INFO mapreduce.Job:  map 100% reduce 38%
18/12/17 22:33:45 INFO mapreduce.Job:  map 100% reduce 40%
18/12/17 22:33:47 INFO mapreduce.Job:  map 100% reduce 41%
18/12/17 22:33:48 INFO mapreduce.Job:  map 100% reduce 44%
18/12/17 22:33:49 INFO mapreduce.Job:  map 100% reduce 50%
18/12/17 22:33:50 INFO mapreduce.Job:  map 100% reduce 52%
18/12/17 22:33:51 INFO mapreduce.Job:  map 100% reduce 58%
18/12/17 22:33:53 INFO mapreduce.Job:  map 100% reduce 59%
18/12/17 22:33:55 INFO mapreduce.Job:  map 100% reduce 61%
18/12/17 22:33:56 INFO mapreduce.Job:  map 100% reduce 63%
18/12/17 22:33:57 INFO mapreduce.Job:  map 100% reduce 65%
18/12/17 22:33:58 INFO mapreduce.Job:  map 100% reduce 67%
18/12/17 22:34:03 INFO mapreduce.Job:  map 100% reduce 68%
18/12/17 22:34:09 INFO mapreduce.Job:  map 100% reduce 69%
18/12/17 22:34:18 INFO mapreduce.Job:  map 100% reduce 70%
18/12/17 22:34:24 INFO mapreduce.Job:  map 100% reduce 71%
18/12/17 22:34:31 INFO mapreduce.Job:  map 100% reduce 72%
18/12/17 22:34:37 INFO mapreduce.Job:  map 100% reduce 73%
18/12/17 22:34:44 INFO mapreduce.Job:  map 100% reduce 74%
18/12/17 22:34:51 INFO mapreduce.Job:  map 100% reduce 75%
18/12/17 22:34:58 INFO mapreduce.Job:  map 100% reduce 76%
18/12/17 22:35:05 INFO mapreduce.Job:  map 100% reduce 77%
18/12/17 22:35:11 INFO mapreduce.Job:  map 100% reduce 78%
18/12/17 22:35:17 INFO mapreduce.Job:  map 100% reduce 79%
18/12/17 22:35:24 INFO mapreduce.Job:  map 100% reduce 80%
18/12/17 22:35:30 INFO mapreduce.Job:  map 100% reduce 81%
18/12/17 22:35:37 INFO mapreduce.Job:  map 100% reduce 82%
18/12/17 22:35:45 INFO mapreduce.Job:  map 100% reduce 83%
18/12/17 22:35:53 INFO mapreduce.Job:  map 100% reduce 84%
18/12/17 22:35:59 INFO mapreduce.Job:  map 100% reduce 85%
18/12/17 22:36:05 INFO mapreduce.Job:  map 100% reduce 86%
18/12/17 22:36:11 INFO mapreduce.Job:  map 100% reduce 87%
18/12/17 22:36:17 INFO mapreduce.Job:  map 100% reduce 88%
18/12/17 22:36:22 INFO mapreduce.Job:  map 100% reduce 89%
18/12/17 22:36:28 INFO mapreduce.Job:  map 100% reduce 90%
18/12/17 22:36:34 INFO mapreduce.Job:  map 100% reduce 91%
18/12/17 22:36:42 INFO mapreduce.Job:  map 100% reduce 92%
18/12/17 22:36:48 INFO mapreduce.Job:  map 100% reduce 93%
18/12/17 22:36:55 INFO mapreduce.Job:  map 100% reduce 94%
18/12/17 22:37:02 INFO mapreduce.Job:  map 100% reduce 95%
18/12/17 22:37:07 INFO mapreduce.Job:  map 100% reduce 96%
18/12/17 22:37:15 INFO mapreduce.Job:  map 100% reduce 97%
18/12/17 22:37:23 INFO mapreduce.Job:  map 100% reduce 98%
18/12/17 22:37:27 INFO mapreduce.Job:  map 100% reduce 99%
18/12/17 22:37:33 INFO mapreduce.Job:  map 100% reduce 100%
18/12/17 22:37:40 INFO mapreduce.Job: Job job_1545055483525_0065 completed successfully
18/12/17 22:37:40 INFO mapreduce.Job: Counters: 51
	File System Counters
		FILE: Number of bytes read=2923216308
		FILE: Number of bytes written=9175571589
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=437719751
		HDFS: Number of bytes written=373448815027
		HDFS: Number of read operations=1560
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=312
	Job Counters 
		Launched map tasks=312
		Launched reduce tasks=156
		Other local map tasks=156
		Data-local map tasks=156
		Total time spent by all maps in occupied slots (ms)=42723177
		Total time spent by all reduces in occupied slots (ms)=115660086
		Total time spent by all map tasks (ms)=14241059
		Total time spent by all reduce tasks (ms)=38553362
		Total vcore-milliseconds taken by all map tasks=14241059
		Total vcore-milliseconds taken by all reduce tasks=38553362
		Total megabyte-milliseconds taken by all map tasks=43748533248
		Total megabyte-milliseconds taken by all reduce tasks=118435928064
	Map-Reduce Framework
		Map input records=10000156
		Map output records=2010000000
		Map output bytes=20640034209
		Map output materialized bytes=6176987065
		Input split bytes=71916
		Combine input records=2010000000
		Combine output records=1137148807
		Reduce input groups=10000000
		Reduce shuffle bytes=6176987065
		Reduce input records=1137148807
		Reduce output records=2000000000
		Spilled Records=2274297614
		Shuffled Maps =48672
		Failed Shuffles=0
		Merged Map outputs=48672
		GC time elapsed (ms)=909843
		CPU time spent (ms)=33892330
		Physical memory (bytes) snapshot=663784288256
		Virtual memory (bytes) snapshot=1517408735232
		Total committed heap usage (bytes)=991507251200
	HiBench.Counters
		BYTES_DATA_GENERATED=350201653627
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=373448815027
18/12/17 22:37:40 INFO HiBench.HiveData: Finished Running Job: Create uservisits
18/12/17 22:37:40 INFO HiBench.HiveData: Closing hive data generator...
rm: `/Scan/Output': No such file or directory
18/12/17 22:37:45 INFO spark.SparkContext: Running Spark version 1.6.0
18/12/17 22:37:46 INFO spark.SecurityManager: Changing view acls to: hdfs
18/12/17 22:37:46 INFO spark.SecurityManager: Changing modify acls to: hdfs
18/12/17 22:37:46 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); users with modify permissions: Set(hdfs)
18/12/17 22:37:46 INFO util.Utils: Successfully started service 'sparkDriver' on port 45208.
18/12/17 22:37:46 INFO slf4j.Slf4jLogger: Slf4jLogger started
18/12/17 22:37:46 INFO Remoting: Starting remoting
18/12/17 22:37:46 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.0.2:37693]
18/12/17 22:37:46 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.0.2:37693]
18/12/17 22:37:46 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 37693.
18/12/17 22:37:46 INFO spark.SparkEnv: Registering MapOutputTracker
18/12/17 22:37:46 INFO spark.SparkEnv: Registering BlockManagerMaster
18/12/17 22:37:46 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-1001c2a7-231c-4090-97dd-7f08a3dd1b7a
18/12/17 22:37:46 INFO storage.MemoryStore: MemoryStore started with capacity 530.3 MB
18/12/17 22:37:47 INFO spark.SparkEnv: Registering OutputCommitCoordinator
18/12/17 22:37:47 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
18/12/17 22:37:47 INFO ui.SparkUI: Started SparkUI at http://10.0.0.2:4040
18/12/17 22:37:47 INFO spark.SparkContext: Added JAR file:/var/lib/hadoop-hdfs/sparkbench-assembly-7.1-SNAPSHOT-dist.jar at spark://10.0.0.2:45208/jars/sparkbench-assembly-7.1-SNAPSHOT-dist.jar with timestamp 1545086267398
18/12/17 22:37:47 INFO yarn.Client: Requesting a new application from cluster with 3 NodeManagers
18/12/17 22:37:47 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
18/12/17 22:37:47 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
18/12/17 22:37:47 INFO yarn.Client: Setting up container launch context for our AM
18/12/17 22:37:47 INFO yarn.Client: Setting up the launch environment for our AM container
18/12/17 22:37:47 INFO yarn.Client: Preparing resources for our AM container
18/12/17 22:37:48 INFO yarn.Client: Uploading resource file:/tmp/spark-ef01c42a-068a-4cf4-8bc0-740257f704cb/__spark_conf__768013160719776744.zip -> hdfs://nameservice1/user/hdfs/.sparkStaging/application_1545055483525_0066/__spark_conf__768013160719776744.zip
18/12/17 22:37:48 INFO spark.SecurityManager: Changing view acls to: hdfs
18/12/17 22:37:48 INFO spark.SecurityManager: Changing modify acls to: hdfs
18/12/17 22:37:48 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); users with modify permissions: Set(hdfs)
18/12/17 22:37:48 INFO yarn.Client: Submitting application 66 to ResourceManager
18/12/17 22:37:49 INFO impl.YarnClientImpl: Submitted application application_1545055483525_0066
18/12/17 22:37:50 INFO yarn.Client: Application report for application_1545055483525_0066 (state: ACCEPTED)
18/12/17 22:37:50 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: root.users.hdfs
	 start time: 1545086268941
	 final status: UNDEFINED
	 tracking URL: http://cdh-master-1.private1.cdhvcn.oraclevcn.com:8088/proxy/application_1545055483525_0066/
	 user: hdfs
18/12/17 22:37:51 INFO yarn.Client: Application report for application_1545055483525_0066 (state: ACCEPTED)
18/12/17 22:37:52 INFO yarn.Client: Application report for application_1545055483525_0066 (state: ACCEPTED)
18/12/17 22:37:53 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(null)
18/12/17 22:37:53 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> cdh-master-1.private1.cdhvcn.oraclevcn.com,cdh-master-2.private2.cdhvcn.oraclevcn.com, PROXY_URI_BASES -> http://cdh-master-1.private1.cdhvcn.oraclevcn.com:8088/proxy/application_1545055483525_0066,http://cdh-master-2.private2.cdhvcn.oraclevcn.com:8088/proxy/application_1545055483525_0066), /proxy/application_1545055483525_0066
18/12/17 22:37:53 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
18/12/17 22:37:53 INFO yarn.Client: Application report for application_1545055483525_0066 (state: ACCEPTED)
18/12/17 22:37:54 INFO yarn.Client: Application report for application_1545055483525_0066 (state: RUNNING)
18/12/17 22:37:54 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 10.0.4.3
	 ApplicationMaster RPC port: 0
	 queue: root.users.hdfs
	 start time: 1545086268941
	 final status: UNDEFINED
	 tracking URL: http://cdh-master-1.private1.cdhvcn.oraclevcn.com:8088/proxy/application_1545055483525_0066/
	 user: hdfs
18/12/17 22:37:54 INFO cluster.YarnClientSchedulerBackend: Application application_1545055483525_0066 has started running.
18/12/17 22:37:54 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34639.
18/12/17 22:37:54 INFO netty.NettyBlockTransferService: Server created on 34639
18/12/17 22:37:54 INFO storage.BlockManager: external shuffle service port = 7337
18/12/17 22:37:54 INFO storage.BlockManagerMaster: Trying to register BlockManager
18/12/17 22:37:54 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.0.2:34639 with 530.3 MB RAM, BlockManagerId(driver, 10.0.0.2, 34639)
18/12/17 22:37:54 INFO storage.BlockManagerMaster: Registered BlockManager
18/12/17 22:37:54 INFO scheduler.EventLoggingListener: Logging events to hdfs://nameservice1/user/spark/applicationHistory/application_1545055483525_0066
18/12/17 22:37:54 INFO spark.SparkContext: Registered listener com.cloudera.spark.lineage.ClouderaNavigatorListener
18/12/17 22:37:54 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
18/12/17 22:37:55 INFO hive.HiveContext: Initializing execution hive, version 1.1.0
18/12/17 22:37:55 INFO client.ClientWrapper: Inspected Hadoop version: 2.6.0-cdh5.16.1
18/12/17 22:37:55 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0-cdh5.16.1
18/12/17 22:37:55 INFO session.SessionState: Created HDFS directory: file:/tmp/spark-ced2bde8-cd85-46c8-80ff-2f7edbc74609/scratch/hdfs
18/12/17 22:37:55 INFO session.SessionState: Created local directory: /tmp/2d4fee39-e8ff-4891-a0ed-bf17b94542e8_resources
18/12/17 22:37:55 INFO session.SessionState: Created HDFS directory: file:/tmp/spark-ced2bde8-cd85-46c8-80ff-2f7edbc74609/scratch/hdfs/2d4fee39-e8ff-4891-a0ed-bf17b94542e8
18/12/17 22:37:55 INFO session.SessionState: Created local directory: /tmp/hdfs/2d4fee39-e8ff-4891-a0ed-bf17b94542e8
18/12/17 22:37:55 INFO session.SessionState: Created HDFS directory: file:/tmp/spark-ced2bde8-cd85-46c8-80ff-2f7edbc74609/scratch/hdfs/2d4fee39-e8ff-4891-a0ed-bf17b94542e8/_tmp_space.db
18/12/17 22:37:55 INFO session.SessionState: No Tez session required at this point. hive.execution.engine=mr.
18/12/17 22:37:56 INFO hive.metastore: Trying to connect to metastore with URI thrift://cdh-utility-1.public1.cdhvcn.oraclevcn.com:9083
18/12/17 22:37:56 INFO hive.metastore: Opened a connection to metastore, current connections: 1
18/12/17 22:37:56 INFO hive.metastore: Connected to metastore.
18/12/17 22:37:56 INFO hive.HiveContext: default warehouse location is /user/hive/warehouse
18/12/17 22:37:56 INFO hive.HiveContext: Initializing metastore client version 1.1.0 using Spark classes.
18/12/17 22:37:56 INFO client.ClientWrapper: Inspected Hadoop version: 2.6.0-cdh5.16.1
18/12/17 22:37:56 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0-cdh5.16.1
18/12/17 22:37:57 INFO session.SessionState: Created local directory: /tmp/1eaec665-bfd4-4956-8b26-e12c15e21e52_resources
18/12/17 22:37:57 INFO session.SessionState: Created HDFS directory: /tmp/hive/hdfs/1eaec665-bfd4-4956-8b26-e12c15e21e52
18/12/17 22:37:57 INFO session.SessionState: Created local directory: /tmp/hdfs/1eaec665-bfd4-4956-8b26-e12c15e21e52
18/12/17 22:37:57 INFO session.SessionState: Created HDFS directory: /tmp/hive/hdfs/1eaec665-bfd4-4956-8b26-e12c15e21e52/_tmp_space.db
18/12/17 22:37:57 INFO session.SessionState: No Tez session required at this point. hive.execution.engine=mr.
18/12/17 22:37:57 INFO hive.metastore: Trying to connect to metastore with URI thrift://cdh-utility-1.public1.cdhvcn.oraclevcn.com:9083
18/12/17 22:37:57 INFO hive.metastore: Opened a connection to metastore, current connections: 1
18/12/17 22:37:57 INFO hive.metastore: Connected to metastore.
18/12/17 22:37:58 INFO log.PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:37:58 INFO log.PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:37:58 INFO log.PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:37:58 INFO ql.Driver: Compiling command(queryId=): USE DEFAULT
18/12/17 22:37:58 INFO log.PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:37:58 INFO log.PerfLogger: </PERFLOG method=parse start=1545086278453 end=1545086278770 duration=317 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:37:58 INFO log.PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:37:58 INFO ql.Driver: Semantic Analysis Completed
18/12/17 22:37:58 INFO log.PerfLogger: </PERFLOG method=semanticAnalyze start=1545086278773 end=1545086278825 duration=52 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:37:58 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
18/12/17 22:37:58 INFO log.PerfLogger: </PERFLOG method=compile start=1545086278418 end=1545086278833 duration=415 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:37:58 INFO log.PerfLogger: </PERFLOG method=compile start=1545086278418 end=1545086278833 duration=415 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:37:58 INFO ql.Driver: Completed compiling command(queryId=); Time taken: 0.415 seconds
18/12/17 22:37:58 INFO log.PerfLogger: <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:37:58 INFO lockmgr.DummyTxnManager: Creating lock manager of type org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager
18/12/17 22:37:58 INFO imps.CuratorFrameworkImpl: Starting
18/12/17 22:37:58 INFO zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.5-1392090, built on 09/30/2012 17:52 GMT
18/12/17 22:37:58 INFO zookeeper.ZooKeeper: Client environment:host.name=cdh-utility-1.public1.cdhvcn.oraclevcn.com
18/12/17 22:37:58 INFO zookeeper.ZooKeeper: Client environment:java.version=1.7.0_67
18/12/17 22:37:58 INFO zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation
18/12/17 22:37:58 INFO zookeeper.ZooKeeper: Client environment:java.home=/usr/java/jdk1.7.0_67-cloudera/jre
18/12/17 22:37:58 INFO zookeeper.ZooKeeper: Client environment:java.class.path=/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/lib/spark/conf/:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/lib/spark/lib/spark-assembly-1.6.0-cdh5.16.1-hadoop2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/lib/spark/conf/yarn-conf/:/etc/hive/conf/:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/ST4-4.0.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/accumulo-core-1.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/accumulo-fate-1.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/accumulo-start-1.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/accumulo-trace-1.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/activation-1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/ant-1.9.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/ant-launcher-1.9.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/antlr-2.7.7.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/antlr-runtime-3.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/aopalliance-1.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/apache-log4j-extras-1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/apache-log4j-extras-1.2.17.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/apacheds-i18n-2.0.0-M15.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/api-asn1-api-1.0.0-M20.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/api-util-1.0.0-M20.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/asm-3.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/asm-commons-3.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/asm-tree-3.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/async-1.4.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/asynchbase-1.7.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-compiler-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-ipc-1.7.6-cdh5.16.1-tests.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-ipc-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-mapred-1.7.6-cdh5.16.1-hadoop2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-maven-plugin-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-protobuf-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-service-archetype-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-thrift-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/aws-java-sdk-bundle-1.11.134.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/azure-data-lake-store-sdk-2.2.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/bonecp-0.8.0.RELEASE.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/calcite-avatica-1.0.0-incubating.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/calcite-core-1.0.0-incubating.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/calcite-linq4j-1.0.0-incubating.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-beanutils-1.9.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-beanutils-core-1.8.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-cli-1.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-codec-1.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-codec-1.8.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-collections-3.2.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-compiler-2.7.6.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-compress-1.4.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-configuration-1.6.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-daemon-1.0.13.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-dbcp-1.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-digester-1.8.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-el-1.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-httpclient-3.0.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-httpclient-3.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-io-2.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-jexl-2.1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-lang-2.6.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-lang3-3.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-logging-1.1.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-math-2.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-math3-3.1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-net-3.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-pool-1.5.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-vfs2-2.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/curator-client-2.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/curator-client-2.7.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/curator-framework-2.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/curator-framework-2.7.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/curator-recipes-2.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/curator-recipes-2.7.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/datanucleus-api-jdo-3.2.6.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/datanucleus-core-3.2.10.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/datanucleus-rdbms-3.2.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/derby-10.11.1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/eigenbase-properties-1.1.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/fastutil-6.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/findbugs-annotations-1.3.9-1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-avro-source-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-dataset-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-file-channel-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-hdfs-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-hive-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-irc-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-jdbc-channel-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-jms-source-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-kafka-channel-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-kafka-source-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-auth-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-config-filter-api-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-configuration-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-core-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-elasticsearch-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-embedded-agent-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-environment-variable-config-filter-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-external-process-config-filter-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-hadoop-credential-store-config-filter-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-hbase-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-kafka-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-log4jappender-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-morphline-solr-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-node-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-sdk-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-scribe-source-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-spillable-memory-channel-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-taildir-source-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-thrift-source-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-tools-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-twitter-source-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/geronimo-annotation_1.0_spec-1.1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/geronimo-jaspic_1.0_spec-1.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/geronimo-jta_1.1_spec-1.1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/groovy-all-2.4.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/gson-2.2.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/guava-11.0.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/guava-11.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/guava-14.0.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/guice-3.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/guice-servlet-3.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-annotations-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-ant-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-archive-logs-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-archives-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-auth-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-aws-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-azure-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-azure-datalake-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-common-2.6.0-cdh5.16.1-tests.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-common-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-datajoin-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-distcp-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-extras-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-gridmix-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-hdfs-2.6.0-cdh5.16.1-tests.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-hdfs-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-hdfs-nfs-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-app-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-common-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-core-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-hs-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-hs-plugins-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.16.1-tests.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-nativetask-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-examples-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-nfs-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-openstack-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-rumen-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-sls-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-streaming-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-api-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-applications-distributedshell-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-client-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-common-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-registry-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-server-applicationhistoryservice-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-server-common-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-server-nodemanager-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-server-resourcemanager-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-server-tests-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-server-web-proxy-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hamcrest-core-1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hamcrest-core-1.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hbase-annotations-1.2.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hbase-client-1.2.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hbase-common-1.2.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hbase-hadoop-compat-1.2.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hbase-hadoop2-compat-1.2.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hbase-protocol-1.2.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hbase-server-1.2.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/high-scale-lib-1.1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-accumulo-handler-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-ant-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-beeline-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-classification-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-cli-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-common-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-contrib-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-exec-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-hbase-handler-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-hwi-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-jdbc-1.1.0-cdh5.16.1-standalone.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-jdbc-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-metastore-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-serde-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-service-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-shims-0.23-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-shims-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-shims-common-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-shims-scheduler-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-testutils-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/htrace-core-3.2.0-incubating.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/htrace-core4-4.0.1-incubating.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/httpclient-4.2.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/httpcore-4.2.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hue-plugins-3.9.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/irclib-1.10.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/ivy-2.0.0-rc2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jackson-annotations-2.2.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jackson-core-2.2.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jackson-core-asl-1.8.10.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jackson-databind-2.2.3-cloudera.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jackson-jaxrs-1.8.10.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jackson-mapper-asl-1.8.10-cloudera.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jackson-xc-1.8.10.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jamon-runtime-2.3.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/janino-2.7.6.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jasper-compiler-5.5.23.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jasper-runtime-5.5.23.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/java-xmlbuilder-0.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/javax.inject-1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jaxb-api-2.2.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jaxb-impl-2.2.3-1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jcommander-1.32.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jdo-api-3.0.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jersey-client-1.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jersey-core-1.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jersey-guice-1.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jersey-json-1.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jersey-server-1.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jets3t-0.6.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jets3t-0.9.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jettison-1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jetty-6.1.26.cloudera.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jetty-all-7.6.0.v20120127.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jetty-all-server-7.6.0.v20120127.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jetty-util-6.1.26.cloudera.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jline-2.11.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jline-2.12.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/joda-time-1.6.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/joda-time-2.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jopt-simple-5.0.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jpam-1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jsch-0.1.42.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jsp-api-2.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jsr305-1.3.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jsr305-3.0.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jta-1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/kafka-clients-0.10.2-kafka-2.2.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/kafka_2.10-0.10.2-kafka-2.2.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/kite-data-core-1.0.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/kite-data-hbase-1.0.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/kite-data-hive-1.0.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/kite-hadoop-compatibility-1.0.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/leveldbjni-all-1.8.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/libfb303-0.9.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/libthrift-0.9.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/log4j-1.2.16.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/log4j-1.2.17.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/logredactor-1.0.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/lz4-1.3.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/mail-1.4.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/mapdb-0.9.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/maven-scm-api-1.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/maven-scm-provider-svn-commons-1.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/maven-scm-provider-svnexe-1.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/metrics-core-2.2.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/metrics-core-3.0.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/metrics-json-3.0.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/metrics-jvm-3.0.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/microsoft-windowsazure-storage-sdk-0.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/mina-core-2.0.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/mockito-all-1.8.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/netty-3.10.5.Final.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/netty-3.9.4.Final.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/netty-all-4.0.23.Final.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/okhttp-2.4.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/okio-1.4.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/opencsv-2.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/oro-2.0.8.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/paranamer-2.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-avro-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-cascading-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-column-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-common-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-encoding-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-format-2.1.0-cdh5.16.1-javadoc.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-format-2.1.0-cdh5.16.1-sources.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-format-2.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-generator-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-hadoop-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-hadoop-bundle-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-jackson-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-pig-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-pig-bundle-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-protobuf-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-scala_2.10-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-scrooge_2.10-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-test-hadoop2-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-thrift-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-tools-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/plexus-utils-1.5.6.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/protobuf-java-2.5.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/regexp-1.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/scala-library-2.10.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/serializer-2.7.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/servlet-api-2.5-20110124.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/servlet-api-2.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/slf4j-api-1.7.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/slf4j-log4j12-1.7.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/snappy-java-1.0.4.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/spark-1.6.0-cdh5.16.1-yarn-shuffle.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/spark-streaming-flume-sink_2.10-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/stax-api-1.0-2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/stax-api-1.0.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/stringtemplate-3.2.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/super-csv-2.2.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/tempus-fugit-1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/trevni-avro-1.7.6-cdh5.16.1-hadoop2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/trevni-avro-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/trevni-core-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/twitter4j-core-3.0.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/twitter4j-media-support-3.0.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/twitter4j-stream-3.0.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/unused-1.0.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/velocity-1.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/velocity-1.7.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/xalan-2.7.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/xercesImpl-2.9.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/xml-apis-1.3.04.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/xmlenc-0.52.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/xz-1.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/zkclient-0.10.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/zookeeper-3.4.5-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/lib/hadoop/LICENSE.txt:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/lib/hadoop/NOTICE.txt
18/12/17 22:37:58 INFO zookeeper.ZooKeeper: Client environment:java.library.path=:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/bin/../lib/hadoop/lib/native:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/lib/hadoop/lib/native:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
18/12/17 22:37:58 INFO zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
18/12/17 22:37:58 INFO zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
18/12/17 22:37:58 INFO zookeeper.ZooKeeper: Client environment:os.name=Linux
18/12/17 22:37:58 INFO zookeeper.ZooKeeper: Client environment:os.arch=amd64
18/12/17 22:37:58 INFO zookeeper.ZooKeeper: Client environment:os.version=3.10.0-862.3.3.el7.x86_64
18/12/17 22:37:58 INFO zookeeper.ZooKeeper: Client environment:user.name=hdfs
18/12/17 22:37:58 INFO zookeeper.ZooKeeper: Client environment:user.home=/var/lib/hadoop-hdfs
18/12/17 22:37:58 INFO zookeeper.ZooKeeper: Client environment:user.dir=/var/lib/hadoop-hdfs/benchmark-baremetal/src
18/12/17 22:37:58 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=cdh-utility-1.public1.cdhvcn.oraclevcn.com:2181,cdh-master-1.private1.cdhvcn.oraclevcn.com:2181,cdh-master-2.private2.cdhvcn.oraclevcn.com:2181 sessionTimeout=1200000 watcher=org.apache.curator.ConnectionState@6c464f46
18/12/17 22:37:58 INFO zookeeper.ClientCnxn: Opening socket connection to server cdh-utility-1.public1.cdhvcn.oraclevcn.com/10.0.0.2:2181. Will not attempt to authenticate using SASL (unknown error)
18/12/17 22:37:58 INFO zookeeper.ClientCnxn: Socket connection established to cdh-utility-1.public1.cdhvcn.oraclevcn.com/10.0.0.2:2181, initiating session
18/12/17 22:37:58 INFO zookeeper.ClientCnxn: Session establishment complete on server cdh-utility-1.public1.cdhvcn.oraclevcn.com/10.0.0.2:2181, sessionid = 0x167bc7ae61a02a4, negotiated timeout = 40000
18/12/17 22:37:58 INFO state.ConnectionStateManager: State change: CONNECTED
18/12/17 22:37:59 INFO log.PerfLogger: </PERFLOG method=acquireReadWriteLocks start=1545086278836 end=1545086279919 duration=1083 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:37:59 INFO log.PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:37:59 INFO ql.Driver: Executing command(queryId=): USE DEFAULT
18/12/17 22:37:59 INFO log.PerfLogger: </PERFLOG method=TimeToSubmit start=1545086278418 end=1545086279924 duration=1506 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:37:59 INFO log.PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:37:59 INFO ql.Driver: Starting task [Stage-0:DDL] in serial mode
18/12/17 22:37:59 INFO log.PerfLogger: </PERFLOG method=runTasks start=1545086279924 end=1545086279934 duration=10 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:37:59 INFO log.PerfLogger: </PERFLOG method=Driver.execute start=1545086279919 end=1545086279934 duration=15 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:37:59 INFO ql.Driver: Completed executing command(queryId=); Time taken: 0.015 seconds
18/12/17 22:37:59 INFO ql.Driver: OK
18/12/17 22:37:59 INFO log.PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:37:59 INFO log.PerfLogger: </PERFLOG method=releaseLocks start=1545086279935 end=1545086279935 duration=0 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:37:59 INFO log.PerfLogger: </PERFLOG method=Driver.run start=1545086278418 end=1545086279935 duration=1517 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:38:00 INFO log.PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:38:00 INFO log.PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:38:00 INFO log.PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:38:00 INFO ql.Driver: Compiling command(queryId=): DROP TABLE IF EXISTS uservisits
18/12/17 22:38:00 INFO log.PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:38:00 INFO log.PerfLogger: </PERFLOG method=parse start=1545086280119 end=1545086280121 duration=2 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:38:00 INFO log.PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:38:00 INFO ql.Driver: Semantic Analysis Completed
18/12/17 22:38:00 INFO log.PerfLogger: </PERFLOG method=semanticAnalyze start=1545086280121 end=1545086280162 duration=41 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:38:00 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
18/12/17 22:38:00 INFO log.PerfLogger: </PERFLOG method=compile start=1545086280119 end=1545086280162 duration=43 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:38:00 INFO log.PerfLogger: </PERFLOG method=compile start=1545086280119 end=1545086280162 duration=43 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:38:00 INFO ql.Driver: Completed compiling command(queryId=); Time taken: 0.043 seconds
18/12/17 22:38:00 INFO log.PerfLogger: <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:38:00 INFO log.PerfLogger: </PERFLOG method=acquireReadWriteLocks start=1545086280163 end=1545086280163 duration=0 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:38:00 INFO log.PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:38:00 INFO ql.Driver: Executing command(queryId=): USE DEFAULT
18/12/17 22:38:00 INFO log.PerfLogger: </PERFLOG method=TimeToSubmit start=1545086280119 end=1545086280163 duration=44 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:38:00 INFO log.PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:38:00 INFO ql.Driver: Starting task [Stage-0:DDL] in serial mode
18/12/17 22:38:00 ERROR metadata.Hive: Table uservisits not found: DEFAULT.uservisits table not found
18/12/17 22:38:00 INFO log.PerfLogger: </PERFLOG method=runTasks start=1545086280163 end=1545086280176 duration=13 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:38:00 INFO log.PerfLogger: </PERFLOG method=Driver.execute start=1545086280163 end=1545086280177 duration=14 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:38:00 INFO ql.Driver: Completed executing command(queryId=); Time taken: 0.014 seconds
18/12/17 22:38:00 INFO ql.Driver: OK
18/12/17 22:38:00 INFO log.PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:38:00 INFO log.PerfLogger: </PERFLOG method=releaseLocks start=1545086280177 end=1545086280177 duration=0 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:38:00 INFO log.PerfLogger: </PERFLOG method=Driver.run start=1545086280119 end=1545086280177 duration=58 from=org.apache.hadoop.hive.ql.Driver>
NoViableAltException(26@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser.type(HiveParser.java:39361)
	at org.apache.hadoop.hive.ql.parse.HiveParser.colType(HiveParser.java:39126)
	at org.apache.hadoop.hive.ql.parse.HiveParser.columnNameType(HiveParser.java:38826)
	at org.apache.hadoop.hive.ql.parse.HiveParser.columnNameTypeList(HiveParser.java:37049)
	at org.apache.hadoop.hive.ql.parse.HiveParser.createTableStatement(HiveParser.java:5059)
	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2559)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1591)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1067)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:205)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:170)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:303)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:295)
	at org.apache.spark.sql.hive.HiveQLDialect$$anonfun$parse$1.apply(HiveContext.scala:66)
	at org.apache.spark.sql.hive.HiveQLDialect$$anonfun$parse$1.apply(HiveContext.scala:66)
	at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1.apply(ClientWrapper.scala:293)
	at org.apache.spark.sql.hive.client.ClientWrapper.liftedTree1$1(ClientWrapper.scala:239)
	at org.apache.spark.sql.hive.client.ClientWrapper.retryLocked(ClientWrapper.scala:238)
	at org.apache.spark.sql.hive.client.ClientWrapper.withHiveState(ClientWrapper.scala:281)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:65)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:211)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:211)
	at org.apache.spark.sql.execution.SparkSQLParser$$anonfun$org$apache$spark$sql$execution$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at org.apache.spark.sql.execution.SparkSQLParser$$anonfun$org$apache$spark$sql$execution$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:113)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:208)
	at org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:208)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:43)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:231)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:334)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:829)
	at com.intel.hibench.sparkbench.sql.ScalaSparkSQLBench$$anonfun$main$1.apply(ScalaSparkSQLBench.scala:43)
	at com.intel.hibench.sparkbench.sql.ScalaSparkSQLBench$$anonfun$main$1.apply(ScalaSparkSQLBench.scala:41)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at com.intel.hibench.sparkbench.sql.ScalaSparkSQLBench$.main(ScalaSparkSQLBench.scala:41)
	at com.intel.hibench.sparkbench.sql.ScalaSparkSQLBench.main(ScalaSparkSQLBench.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:730)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Exception in thread "main" org.apache.spark.sql.AnalysisException: cannot recognize input near 'archWord' 'STRING' ',' in column type; line 2 pos 0
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:318)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:295)
	at org.apache.spark.sql.hive.HiveQLDialect$$anonfun$parse$1.apply(HiveContext.scala:66)
	at org.apache.spark.sql.hive.HiveQLDialect$$anonfun$parse$1.apply(HiveContext.scala:66)
	at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1.apply(ClientWrapper.scala:293)
	at org.apache.spark.sql.hive.client.ClientWrapper.liftedTree1$1(ClientWrapper.scala:239)
	at org.apache.spark.sql.hive.client.ClientWrapper.retryLocked(ClientWrapper.scala:238)
	at org.apache.spark.sql.hive.client.ClientWrapper.withHiveState(ClientWrapper.scala:281)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:65)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:211)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:211)
	at org.apache.spark.sql.execution.SparkSQLParser$$anonfun$org$apache$spark$sql$execution$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at org.apache.spark.sql.execution.SparkSQLParser$$anonfun$org$apache$spark$sql$execution$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:113)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:208)
	at org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:208)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:43)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:231)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:334)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:829)
	at com.intel.hibench.sparkbench.sql.ScalaSparkSQLBench$$anonfun$main$1.apply(ScalaSparkSQLBench.scala:43)
	at com.intel.hibench.sparkbench.sql.ScalaSparkSQLBench$$anonfun$main$1.apply(ScalaSparkSQLBench.scala:41)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at com.intel.hibench.sparkbench.sql.ScalaSparkSQLBench$.main(ScalaSparkSQLBench.scala:41)
	at com.intel.hibench.sparkbench.sql.ScalaSparkSQLBench.main(ScalaSparkSQLBench.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:730)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
18/12/17 22:38:00 INFO spark.SparkContext: Invoking stop() from shutdown hook
18/12/17 22:38:00 INFO zookeeper.ZooKeeper: Session: 0x167bc7ae61a02a4 closed
18/12/17 22:38:00 INFO zookeeper.ClientCnxn: EventThread shut down
18/12/17 22:38:00 INFO CuratorFrameworkSingleton: Closing ZooKeeper client.
18/12/17 22:38:00 INFO ui.SparkUI: Stopped Spark web UI at http://10.0.0.2:4040
18/12/17 22:38:00 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread
18/12/17 22:38:00 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors
18/12/17 22:38:00 INFO cluster.YarnClientSchedulerBackend: Asking each executor to shut down
18/12/17 22:38:00 INFO cluster.YarnClientSchedulerBackend: Stopped
18/12/17 22:38:00 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/12/17 22:38:00 INFO storage.MemoryStore: MemoryStore cleared
18/12/17 22:38:00 INFO storage.BlockManager: BlockManager stopped
18/12/17 22:38:00 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
18/12/17 22:38:00 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/12/17 22:38:00 INFO spark.SparkContext: Successfully stopped SparkContext
18/12/17 22:38:00 INFO util.ShutdownHookManager: Shutdown hook called
18/12/17 22:38:00 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-ef01c42a-068a-4cf4-8bc0-740257f704cb
18/12/17 22:38:00 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-ced2bde8-cd85-46c8-80ff-2f7edbc74609
18/12/17 22:38:00 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
18/12/17 22:38:00 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
