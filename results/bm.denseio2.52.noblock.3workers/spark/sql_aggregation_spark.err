rm: `/Aggregation/Input': No such file or directory
18/12/17 22:03:25 INFO HiBench.HiveData: Generating hive data files...
18/12/17 22:03:25 INFO HiBench.HiveData: Initializing hive date generator...
18/12/17 22:03:28 INFO HiBench.Dummy: Creating dummy file /Aggregation/temp/dummy with 156 slots...
18/12/17 22:03:28 INFO HiBench.HiveData: Creating table rankings...
18/12/17 22:03:28 INFO Configuration.deprecation: mapred.reduce.slowstart.completed.maps is deprecated. Instead, use mapreduce.job.reduce.slowstart.completedmaps
18/12/17 22:03:28 INFO HiBench.HiveData: Running Job: Create rankings
18/12/17 22:03:28 INFO HiBench.HiveData: Pages file /Aggregation/temp/dummy as input
18/12/17 22:03:28 INFO HiBench.HiveData: Rankings file /Aggregation/Input/rankings as output
18/12/17 22:03:28 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
18/12/17 22:03:29 INFO mapred.FileInputFormat: Total input paths to process : 1
18/12/17 22:03:29 INFO mapreduce.JobSubmitter: number of splits:156
18/12/17 22:03:29 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1545055483525_0058
18/12/17 22:03:29 INFO impl.YarnClientImpl: Submitted application application_1545055483525_0058
18/12/17 22:03:29 INFO mapreduce.Job: The url to track the job: http://cdh-master-1.private1.cdhvcn.oraclevcn.com:8088/proxy/application_1545055483525_0058/
18/12/17 22:03:29 INFO mapreduce.Job: Running job: job_1545055483525_0058
18/12/17 22:03:41 INFO mapreduce.Job: Job job_1545055483525_0058 running in uber mode : false
18/12/17 22:03:41 INFO mapreduce.Job:  map 0% reduce 0%
18/12/17 22:05:03 INFO mapreduce.Job:  map 45% reduce 0%
18/12/17 22:05:07 INFO mapreduce.Job:  map 67% reduce 0%
18/12/17 22:06:26 INFO mapreduce.Job:  map 68% reduce 0%
18/12/17 22:06:28 INFO mapreduce.Job:  map 70% reduce 0%
18/12/17 22:06:29 INFO mapreduce.Job:  map 72% reduce 0%
18/12/17 22:06:30 INFO mapreduce.Job:  map 74% reduce 0%
18/12/17 22:06:31 INFO mapreduce.Job:  map 75% reduce 0%
18/12/17 22:06:32 INFO mapreduce.Job:  map 76% reduce 0%
18/12/17 22:06:34 INFO mapreduce.Job:  map 77% reduce 0%
18/12/17 22:06:46 INFO mapreduce.Job:  map 78% reduce 0%
18/12/17 22:06:57 INFO mapreduce.Job:  map 78% reduce 7%
18/12/17 22:06:58 INFO mapreduce.Job:  map 78% reduce 8%
18/12/17 22:06:59 INFO mapreduce.Job:  map 78% reduce 10%
18/12/17 22:07:00 INFO mapreduce.Job:  map 78% reduce 11%
18/12/17 22:07:35 INFO mapreduce.Job:  map 80% reduce 11%
18/12/17 22:07:36 INFO mapreduce.Job:  map 81% reduce 11%
18/12/17 22:07:37 INFO mapreduce.Job:  map 83% reduce 12%
18/12/17 22:07:38 INFO mapreduce.Job:  map 85% reduce 12%
18/12/17 22:07:39 INFO mapreduce.Job:  map 87% reduce 12%
18/12/17 22:07:40 INFO mapreduce.Job:  map 90% reduce 15%
18/12/17 22:07:41 INFO mapreduce.Job:  map 91% reduce 19%
18/12/17 22:07:42 INFO mapreduce.Job:  map 93% reduce 19%
18/12/17 22:07:43 INFO mapreduce.Job:  map 96% reduce 21%
18/12/17 22:07:44 INFO mapreduce.Job:  map 98% reduce 22%
18/12/17 22:07:45 INFO mapreduce.Job:  map 99% reduce 23%
18/12/17 22:07:46 INFO mapreduce.Job:  map 100% reduce 26%
18/12/17 22:07:47 INFO mapreduce.Job:  map 100% reduce 31%
18/12/17 22:07:49 INFO mapreduce.Job:  map 100% reduce 32%
18/12/17 22:07:50 INFO mapreduce.Job:  map 100% reduce 33%
18/12/17 22:07:52 INFO mapreduce.Job:  map 100% reduce 34%
18/12/17 22:07:53 INFO mapreduce.Job:  map 100% reduce 36%
18/12/17 22:07:55 INFO mapreduce.Job:  map 100% reduce 38%
18/12/17 22:07:56 INFO mapreduce.Job:  map 100% reduce 39%
18/12/17 22:07:57 INFO mapreduce.Job:  map 100% reduce 40%
18/12/17 22:07:58 INFO mapreduce.Job:  map 100% reduce 42%
18/12/17 22:07:59 INFO mapreduce.Job:  map 100% reduce 46%
18/12/17 22:08:00 INFO mapreduce.Job:  map 100% reduce 47%
18/12/17 22:08:01 INFO mapreduce.Job:  map 100% reduce 48%
18/12/17 22:08:02 INFO mapreduce.Job:  map 100% reduce 50%
18/12/17 22:08:03 INFO mapreduce.Job:  map 100% reduce 51%
18/12/17 22:08:04 INFO mapreduce.Job:  map 100% reduce 52%
18/12/17 22:08:05 INFO mapreduce.Job:  map 100% reduce 56%
18/12/17 22:08:06 INFO mapreduce.Job:  map 100% reduce 58%
18/12/17 22:08:07 INFO mapreduce.Job:  map 100% reduce 59%
18/12/17 22:08:08 INFO mapreduce.Job:  map 100% reduce 61%
18/12/17 22:08:09 INFO mapreduce.Job:  map 100% reduce 62%
18/12/17 22:08:10 INFO mapreduce.Job:  map 100% reduce 63%
18/12/17 22:08:11 INFO mapreduce.Job:  map 100% reduce 66%
18/12/17 22:08:12 INFO mapreduce.Job:  map 100% reduce 70%
18/12/17 22:08:14 INFO mapreduce.Job:  map 100% reduce 74%
18/12/17 22:08:16 INFO mapreduce.Job:  map 100% reduce 75%
18/12/17 22:08:17 INFO mapreduce.Job:  map 100% reduce 77%
18/12/17 22:08:18 INFO mapreduce.Job:  map 100% reduce 82%
18/12/17 22:08:22 INFO mapreduce.Job:  map 100% reduce 83%
18/12/17 22:08:23 INFO mapreduce.Job:  map 100% reduce 85%
18/12/17 22:08:24 INFO mapreduce.Job:  map 100% reduce 89%
18/12/17 22:08:29 INFO mapreduce.Job:  map 100% reduce 91%
18/12/17 22:08:30 INFO mapreduce.Job:  map 100% reduce 96%
18/12/17 22:08:31 INFO mapreduce.Job:  map 100% reduce 97%
18/12/17 22:08:33 INFO mapreduce.Job:  map 100% reduce 99%
18/12/17 22:08:34 INFO mapreduce.Job:  map 100% reduce 100%
18/12/17 22:08:39 INFO mapreduce.Job: Job job_1545055483525_0058 completed successfully
18/12/17 22:08:39 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=10713514630
		FILE: Number of bytes written=32547248013
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=58808
		HDFS: Number of bytes written=4374815075
		HDFS: Number of read operations=936
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=312
	Job Counters 
		Launched map tasks=156
		Launched reduce tasks=156
		Other local map tasks=156
		Total time spent by all maps in occupied slots (ms)=100075926
		Total time spent by all reduces in occupied slots (ms)=52037469
		Total time spent by all map tasks (ms)=33358642
		Total time spent by all reduce tasks (ms)=17345823
		Total vcore-milliseconds taken by all map tasks=33358642
		Total vcore-milliseconds taken by all reduce tasks=17345823
		Total megabyte-milliseconds taken by all map tasks=102477748224
		Total megabyte-milliseconds taken by all reduce tasks=53286368256
	Map-Reduce Framework
		Map input records=156
		Map output records=3316573038
		Map output bytes=38565765923
		Map output materialized bytes=21783856815
		Input split bytes=14664
		Combine input records=3316573038
		Combine output records=3295950853
		Reduce input groups=100000000
		Reduce shuffle bytes=21783856815
		Reduce input records=3295950853
		Reduce output records=100000000
		Spilled Records=6591901706
		Shuffled Maps =24336
		Failed Shuffles=0
		Merged Map outputs=24336
		GC time elapsed (ms)=3898862
		CPU time spent (ms)=30054880
		Physical memory (bytes) snapshot=512368656384
		Virtual memory (bytes) snapshot=1011988152320
		Total committed heap usage (bytes)=652348489728
	HiBench.Counters
		BYTES_DATA_GENERATED=6894731548
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=44144
	File Output Format Counters 
		Bytes Written=4374815075
18/12/17 22:08:39 INFO HiBench.HiveData: Finished Running Job: Create rankings
18/12/17 22:08:39 INFO HiBench.HiveData: Creating user visits...
18/12/17 22:08:39 INFO HiBench.HiveData: Running Job: Create uservisits
18/12/17 22:08:39 INFO HiBench.HiveData: Dummy file /Aggregation/temp/dummy as input
18/12/17 22:08:39 INFO HiBench.HiveData: Rankings file /Aggregation/Input/rankings as input
18/12/17 22:08:39 INFO HiBench.HiveData: Ouput file /Aggregation/Input/uservisits
18/12/17 22:08:40 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
18/12/17 22:08:40 INFO mapred.FileInputFormat: Total input paths to process : 156
18/12/17 22:08:40 INFO mapred.FileInputFormat: Total input paths to process : 1
18/12/17 22:08:40 INFO mapreduce.JobSubmitter: number of splits:312
18/12/17 22:08:40 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1545055483525_0059
18/12/17 22:08:40 INFO impl.YarnClientImpl: Submitted application application_1545055483525_0059
18/12/17 22:08:40 INFO mapreduce.Job: The url to track the job: http://cdh-master-1.private1.cdhvcn.oraclevcn.com:8088/proxy/application_1545055483525_0059/
18/12/17 22:08:40 INFO mapreduce.Job: Running job: job_1545055483525_0059
18/12/17 22:08:47 INFO mapreduce.Job: Job job_1545055483525_0059 running in uber mode : false
18/12/17 22:08:47 INFO mapreduce.Job:  map 0% reduce 0%
18/12/17 22:09:09 INFO mapreduce.Job:  map 4% reduce 0%
18/12/17 22:09:10 INFO mapreduce.Job:  map 18% reduce 0%
18/12/17 22:09:11 INFO mapreduce.Job:  map 59% reduce 0%
18/12/17 22:09:12 INFO mapreduce.Job:  map 83% reduce 0%
18/12/17 22:09:26 INFO mapreduce.Job:  map 84% reduce 0%
18/12/17 22:09:27 INFO mapreduce.Job:  map 85% reduce 0%
18/12/17 22:09:29 INFO mapreduce.Job:  map 87% reduce 0%
18/12/17 22:09:30 INFO mapreduce.Job:  map 88% reduce 0%
18/12/17 22:09:31 INFO mapreduce.Job:  map 89% reduce 0%
18/12/17 22:09:35 INFO mapreduce.Job:  map 90% reduce 0%
18/12/17 22:09:36 INFO mapreduce.Job:  map 92% reduce 0%
18/12/17 22:09:37 INFO mapreduce.Job:  map 95% reduce 0%
18/12/17 22:09:38 INFO mapreduce.Job:  map 98% reduce 0%
18/12/17 22:09:39 INFO mapreduce.Job:  map 99% reduce 0%
18/12/17 22:09:41 INFO mapreduce.Job:  map 100% reduce 0%
18/12/17 22:09:55 INFO mapreduce.Job:  map 100% reduce 1%
18/12/17 22:09:56 INFO mapreduce.Job:  map 100% reduce 8%
18/12/17 22:10:02 INFO mapreduce.Job:  map 100% reduce 13%
18/12/17 22:10:03 INFO mapreduce.Job:  map 100% reduce 23%
18/12/17 22:10:04 INFO mapreduce.Job:  map 100% reduce 25%
18/12/17 22:10:05 INFO mapreduce.Job:  map 100% reduce 26%
18/12/17 22:10:06 INFO mapreduce.Job:  map 100% reduce 34%
18/12/17 22:10:08 INFO mapreduce.Job:  map 100% reduce 35%
18/12/17 22:10:09 INFO mapreduce.Job:  map 100% reduce 38%
18/12/17 22:10:10 INFO mapreduce.Job:  map 100% reduce 39%
18/12/17 22:10:11 INFO mapreduce.Job:  map 100% reduce 40%
18/12/17 22:10:12 INFO mapreduce.Job:  map 100% reduce 43%
18/12/17 22:10:14 INFO mapreduce.Job:  map 100% reduce 45%
18/12/17 22:10:15 INFO mapreduce.Job:  map 100% reduce 50%
18/12/17 22:10:16 INFO mapreduce.Job:  map 100% reduce 54%
18/12/17 22:10:17 INFO mapreduce.Job:  map 100% reduce 56%
18/12/17 22:10:18 INFO mapreduce.Job:  map 100% reduce 57%
18/12/17 22:10:19 INFO mapreduce.Job:  map 100% reduce 61%
18/12/17 22:10:20 INFO mapreduce.Job:  map 100% reduce 62%
18/12/17 22:10:21 INFO mapreduce.Job:  map 100% reduce 63%
18/12/17 22:10:22 INFO mapreduce.Job:  map 100% reduce 66%
18/12/17 22:10:23 INFO mapreduce.Job:  map 100% reduce 67%
18/12/17 22:10:24 INFO mapreduce.Job:  map 100% reduce 68%
18/12/17 22:10:25 INFO mapreduce.Job:  map 100% reduce 69%
18/12/17 22:10:26 INFO mapreduce.Job:  map 100% reduce 70%
18/12/17 22:10:29 INFO mapreduce.Job:  map 100% reduce 71%
18/12/17 22:10:35 INFO mapreduce.Job:  map 100% reduce 72%
18/12/17 22:10:39 INFO mapreduce.Job:  map 100% reduce 73%
18/12/17 22:10:43 INFO mapreduce.Job:  map 100% reduce 74%
18/12/17 22:10:48 INFO mapreduce.Job:  map 100% reduce 75%
18/12/17 22:10:52 INFO mapreduce.Job:  map 100% reduce 76%
18/12/17 22:10:55 INFO mapreduce.Job:  map 100% reduce 77%
18/12/17 22:10:59 INFO mapreduce.Job:  map 100% reduce 78%
18/12/17 22:11:02 INFO mapreduce.Job:  map 100% reduce 79%
18/12/17 22:11:05 INFO mapreduce.Job:  map 100% reduce 80%
18/12/17 22:11:09 INFO mapreduce.Job:  map 100% reduce 81%
18/12/17 22:11:12 INFO mapreduce.Job:  map 100% reduce 82%
18/12/17 22:11:15 INFO mapreduce.Job:  map 100% reduce 83%
18/12/17 22:11:18 INFO mapreduce.Job:  map 100% reduce 84%
18/12/17 22:11:22 INFO mapreduce.Job:  map 100% reduce 85%
18/12/17 22:11:25 INFO mapreduce.Job:  map 100% reduce 86%
18/12/17 22:11:28 INFO mapreduce.Job:  map 100% reduce 87%
18/12/17 22:11:31 INFO mapreduce.Job:  map 100% reduce 88%
18/12/17 22:11:35 INFO mapreduce.Job:  map 100% reduce 89%
18/12/17 22:11:39 INFO mapreduce.Job:  map 100% reduce 90%
18/12/17 22:11:42 INFO mapreduce.Job:  map 100% reduce 91%
18/12/17 22:11:44 INFO mapreduce.Job:  map 100% reduce 92%
18/12/17 22:11:48 INFO mapreduce.Job:  map 100% reduce 93%
18/12/17 22:11:51 INFO mapreduce.Job:  map 100% reduce 94%
18/12/17 22:11:54 INFO mapreduce.Job:  map 100% reduce 95%
18/12/17 22:11:57 INFO mapreduce.Job:  map 100% reduce 96%
18/12/17 22:12:00 INFO mapreduce.Job:  map 100% reduce 97%
18/12/17 22:12:03 INFO mapreduce.Job:  map 100% reduce 98%
18/12/17 22:12:05 INFO mapreduce.Job:  map 100% reduce 99%
18/12/17 22:12:08 INFO mapreduce.Job:  map 100% reduce 100%
18/12/17 22:12:12 INFO mapreduce.Job: Job job_1545055483525_0059 completed successfully
18/12/17 22:12:12 INFO mapreduce.Job: Counters: 51
	File System Counters
		FILE: Number of bytes read=7875512402
		FILE: Number of bytes written=19080163836
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=4374933319
		HDFS: Number of bytes written=186720286085
		HDFS: Number of read operations=1560
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=312
	Job Counters 
		Launched map tasks=312
		Launched reduce tasks=156
		Other local map tasks=156
		Data-local map tasks=156
		Total time spent by all maps in occupied slots (ms)=30676083
		Total time spent by all reduces in occupied slots (ms)=62337888
		Total time spent by all map tasks (ms)=10225361
		Total time spent by all reduce tasks (ms)=20779296
		Total vcore-milliseconds taken by all map tasks=10225361
		Total vcore-milliseconds taken by all reduce tasks=20779296
		Total megabyte-milliseconds taken by all map tasks=31412308992
		Total megabyte-milliseconds taken by all reduce tasks=63833997312
	Map-Reduce Framework
		Map input records=100000156
		Map output records=1100000000
		Map output bytes=16400017487
		Map output materialized bytes=11129256542
		Input split bytes=74100
		Combine input records=1100000000
		Combine output records=1068630667
		Reduce input groups=100000000
		Reduce shuffle bytes=11129256542
		Reduce input records=1068630667
		Reduce output records=1000000000
		Spilled Records=2137261334
		Shuffled Maps =48672
		Failed Shuffles=0
		Merged Map outputs=48672
		GC time elapsed (ms)=587752
		CPU time spent (ms)=20778620
		Physical memory (bytes) snapshot=680228626432
		Virtual memory (bytes) snapshot=1517106208768
		Total committed heap usage (bytes)=989204054016
	HiBench.Counters
		BYTES_DATA_GENERATED=175096850594
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=186720286085
18/12/17 22:12:12 INFO HiBench.HiveData: Finished Running Job: Create uservisits
18/12/17 22:12:12 INFO HiBench.HiveData: Closing hive data generator...
rm: `/Aggregation/Output': No such file or directory
18/12/17 22:12:17 INFO spark.SparkContext: Running Spark version 1.6.0
18/12/17 22:12:17 INFO spark.SecurityManager: Changing view acls to: hdfs
18/12/17 22:12:17 INFO spark.SecurityManager: Changing modify acls to: hdfs
18/12/17 22:12:17 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); users with modify permissions: Set(hdfs)
18/12/17 22:12:17 INFO util.Utils: Successfully started service 'sparkDriver' on port 33319.
18/12/17 22:12:18 INFO slf4j.Slf4jLogger: Slf4jLogger started
18/12/17 22:12:18 INFO Remoting: Starting remoting
18/12/17 22:12:18 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.0.2:36110]
18/12/17 22:12:18 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@10.0.0.2:36110]
18/12/17 22:12:18 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 36110.
18/12/17 22:12:18 INFO spark.SparkEnv: Registering MapOutputTracker
18/12/17 22:12:18 INFO spark.SparkEnv: Registering BlockManagerMaster
18/12/17 22:12:18 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-e9dda2b9-67ec-4153-b015-b680cf48fc6c
18/12/17 22:12:18 INFO storage.MemoryStore: MemoryStore started with capacity 530.3 MB
18/12/17 22:12:18 INFO spark.SparkEnv: Registering OutputCommitCoordinator
18/12/17 22:12:18 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
18/12/17 22:12:18 INFO ui.SparkUI: Started SparkUI at http://10.0.0.2:4040
18/12/17 22:12:18 INFO spark.SparkContext: Added JAR file:/var/lib/hadoop-hdfs/sparkbench-assembly-7.1-SNAPSHOT-dist.jar at spark://10.0.0.2:33319/jars/sparkbench-assembly-7.1-SNAPSHOT-dist.jar with timestamp 1545084738925
18/12/17 22:12:19 INFO yarn.Client: Requesting a new application from cluster with 3 NodeManagers
18/12/17 22:12:19 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
18/12/17 22:12:19 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
18/12/17 22:12:19 INFO yarn.Client: Setting up container launch context for our AM
18/12/17 22:12:19 INFO yarn.Client: Setting up the launch environment for our AM container
18/12/17 22:12:19 INFO yarn.Client: Preparing resources for our AM container
18/12/17 22:12:20 INFO yarn.Client: Uploading resource file:/tmp/spark-45e00120-76b4-4c30-adef-d9c05a52b8c7/__spark_conf__1960181235509416919.zip -> hdfs://nameservice1/user/hdfs/.sparkStaging/application_1545055483525_0060/__spark_conf__1960181235509416919.zip
18/12/17 22:12:20 INFO spark.SecurityManager: Changing view acls to: hdfs
18/12/17 22:12:20 INFO spark.SecurityManager: Changing modify acls to: hdfs
18/12/17 22:12:20 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hdfs); users with modify permissions: Set(hdfs)
18/12/17 22:12:20 INFO yarn.Client: Submitting application 60 to ResourceManager
18/12/17 22:12:20 INFO impl.YarnClientImpl: Submitted application application_1545055483525_0060
18/12/17 22:12:21 INFO yarn.Client: Application report for application_1545055483525_0060 (state: ACCEPTED)
18/12/17 22:12:21 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: root.users.hdfs
	 start time: 1545084740472
	 final status: UNDEFINED
	 tracking URL: http://cdh-master-1.private1.cdhvcn.oraclevcn.com:8088/proxy/application_1545055483525_0060/
	 user: hdfs
18/12/17 22:12:22 INFO yarn.Client: Application report for application_1545055483525_0060 (state: ACCEPTED)
18/12/17 22:12:23 INFO yarn.Client: Application report for application_1545055483525_0060 (state: ACCEPTED)
18/12/17 22:12:24 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(null)
18/12/17 22:12:24 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> cdh-master-1.private1.cdhvcn.oraclevcn.com,cdh-master-2.private2.cdhvcn.oraclevcn.com, PROXY_URI_BASES -> http://cdh-master-1.private1.cdhvcn.oraclevcn.com:8088/proxy/application_1545055483525_0060,http://cdh-master-2.private2.cdhvcn.oraclevcn.com:8088/proxy/application_1545055483525_0060), /proxy/application_1545055483525_0060
18/12/17 22:12:24 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
18/12/17 22:12:24 INFO yarn.Client: Application report for application_1545055483525_0060 (state: ACCEPTED)
18/12/17 22:12:25 INFO yarn.Client: Application report for application_1545055483525_0060 (state: RUNNING)
18/12/17 22:12:25 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 10.0.3.3
	 ApplicationMaster RPC port: 0
	 queue: root.users.hdfs
	 start time: 1545084740472
	 final status: UNDEFINED
	 tracking URL: http://cdh-master-1.private1.cdhvcn.oraclevcn.com:8088/proxy/application_1545055483525_0060/
	 user: hdfs
18/12/17 22:12:25 INFO cluster.YarnClientSchedulerBackend: Application application_1545055483525_0060 has started running.
18/12/17 22:12:25 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34086.
18/12/17 22:12:25 INFO netty.NettyBlockTransferService: Server created on 34086
18/12/17 22:12:25 INFO storage.BlockManager: external shuffle service port = 7337
18/12/17 22:12:25 INFO storage.BlockManagerMaster: Trying to register BlockManager
18/12/17 22:12:25 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.0.2:34086 with 530.3 MB RAM, BlockManagerId(driver, 10.0.0.2, 34086)
18/12/17 22:12:25 INFO storage.BlockManagerMaster: Registered BlockManager
18/12/17 22:12:26 INFO scheduler.EventLoggingListener: Logging events to hdfs://nameservice1/user/spark/applicationHistory/application_1545055483525_0060
18/12/17 22:12:26 INFO spark.SparkContext: Registered listener com.cloudera.spark.lineage.ClouderaNavigatorListener
18/12/17 22:12:26 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
18/12/17 22:12:26 INFO hive.HiveContext: Initializing execution hive, version 1.1.0
18/12/17 22:12:26 INFO client.ClientWrapper: Inspected Hadoop version: 2.6.0-cdh5.16.1
18/12/17 22:12:26 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0-cdh5.16.1
18/12/17 22:12:27 INFO session.SessionState: Created HDFS directory: file:/tmp/spark-9fee23e0-45b6-40e0-bf9c-c4e19c56b679/scratch/hdfs
18/12/17 22:12:27 INFO session.SessionState: Created local directory: /tmp/hdfs
18/12/17 22:12:27 INFO session.SessionState: Created local directory: /tmp/c8af8d99-2c43-430e-a279-7943470f2915_resources
18/12/17 22:12:27 INFO session.SessionState: Created HDFS directory: file:/tmp/spark-9fee23e0-45b6-40e0-bf9c-c4e19c56b679/scratch/hdfs/c8af8d99-2c43-430e-a279-7943470f2915
18/12/17 22:12:27 INFO session.SessionState: Created local directory: /tmp/hdfs/c8af8d99-2c43-430e-a279-7943470f2915
18/12/17 22:12:27 INFO session.SessionState: Created HDFS directory: file:/tmp/spark-9fee23e0-45b6-40e0-bf9c-c4e19c56b679/scratch/hdfs/c8af8d99-2c43-430e-a279-7943470f2915/_tmp_space.db
18/12/17 22:12:27 INFO session.SessionState: No Tez session required at this point. hive.execution.engine=mr.
18/12/17 22:12:27 INFO hive.metastore: Trying to connect to metastore with URI thrift://cdh-utility-1.public1.cdhvcn.oraclevcn.com:9083
18/12/17 22:12:27 INFO hive.metastore: Opened a connection to metastore, current connections: 1
18/12/17 22:12:27 INFO hive.metastore: Connected to metastore.
18/12/17 22:12:27 INFO hive.HiveContext: default warehouse location is /user/hive/warehouse
18/12/17 22:12:27 INFO hive.HiveContext: Initializing metastore client version 1.1.0 using Spark classes.
18/12/17 22:12:27 INFO client.ClientWrapper: Inspected Hadoop version: 2.6.0-cdh5.16.1
18/12/17 22:12:27 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0-cdh5.16.1
18/12/17 22:12:28 INFO session.SessionState: Created HDFS directory: /tmp/hive/hdfs
18/12/17 22:12:28 INFO session.SessionState: Created local directory: /tmp/b0623016-4f33-4c3e-9489-7b5710c9f616_resources
18/12/17 22:12:28 INFO session.SessionState: Created HDFS directory: /tmp/hive/hdfs/b0623016-4f33-4c3e-9489-7b5710c9f616
18/12/17 22:12:28 INFO session.SessionState: Created local directory: /tmp/hdfs/b0623016-4f33-4c3e-9489-7b5710c9f616
18/12/17 22:12:28 INFO session.SessionState: Created HDFS directory: /tmp/hive/hdfs/b0623016-4f33-4c3e-9489-7b5710c9f616/_tmp_space.db
18/12/17 22:12:28 INFO session.SessionState: No Tez session required at this point. hive.execution.engine=mr.
18/12/17 22:12:28 INFO hive.metastore: Trying to connect to metastore with URI thrift://cdh-utility-1.public1.cdhvcn.oraclevcn.com:9083
18/12/17 22:12:28 INFO hive.metastore: Opened a connection to metastore, current connections: 1
18/12/17 22:12:28 INFO hive.metastore: Connected to metastore.
18/12/17 22:12:29 INFO log.PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:29 INFO log.PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:29 INFO log.PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:29 INFO ql.Driver: Compiling command(queryId=): USE DEFAULT
18/12/17 22:12:29 INFO log.PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:30 INFO log.PerfLogger: </PERFLOG method=parse start=1545084749952 end=1545084750272 duration=320 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:30 INFO log.PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:30 INFO ql.Driver: Semantic Analysis Completed
18/12/17 22:12:30 INFO log.PerfLogger: </PERFLOG method=semanticAnalyze start=1545084750275 end=1545084750327 duration=52 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:30 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
18/12/17 22:12:30 INFO log.PerfLogger: </PERFLOG method=compile start=1545084749918 end=1545084750334 duration=416 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:30 INFO log.PerfLogger: </PERFLOG method=compile start=1545084749918 end=1545084750335 duration=417 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:30 INFO ql.Driver: Completed compiling command(queryId=); Time taken: 0.416 seconds
18/12/17 22:12:30 INFO log.PerfLogger: <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:30 INFO lockmgr.DummyTxnManager: Creating lock manager of type org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager
18/12/17 22:12:30 INFO imps.CuratorFrameworkImpl: Starting
18/12/17 22:12:30 INFO zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.5-1392090, built on 09/30/2012 17:52 GMT
18/12/17 22:12:30 INFO zookeeper.ZooKeeper: Client environment:host.name=cdh-utility-1.public1.cdhvcn.oraclevcn.com
18/12/17 22:12:30 INFO zookeeper.ZooKeeper: Client environment:java.version=1.7.0_67
18/12/17 22:12:30 INFO zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation
18/12/17 22:12:30 INFO zookeeper.ZooKeeper: Client environment:java.home=/usr/java/jdk1.7.0_67-cloudera/jre
18/12/17 22:12:30 INFO zookeeper.ZooKeeper: Client environment:java.class.path=/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/lib/spark/conf/:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/lib/spark/lib/spark-assembly-1.6.0-cdh5.16.1-hadoop2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/lib/spark/conf/yarn-conf/:/etc/hive/conf/:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/ST4-4.0.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/accumulo-core-1.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/accumulo-fate-1.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/accumulo-start-1.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/accumulo-trace-1.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/activation-1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/ant-1.9.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/ant-launcher-1.9.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/antlr-2.7.7.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/antlr-runtime-3.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/aopalliance-1.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/apache-log4j-extras-1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/apache-log4j-extras-1.2.17.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/apacheds-i18n-2.0.0-M15.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/api-asn1-api-1.0.0-M20.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/api-util-1.0.0-M20.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/asm-3.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/asm-commons-3.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/asm-tree-3.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/async-1.4.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/asynchbase-1.7.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-compiler-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-ipc-1.7.6-cdh5.16.1-tests.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-ipc-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-mapred-1.7.6-cdh5.16.1-hadoop2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-maven-plugin-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-protobuf-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-service-archetype-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/avro-thrift-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/aws-java-sdk-bundle-1.11.134.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/azure-data-lake-store-sdk-2.2.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/bonecp-0.8.0.RELEASE.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/calcite-avatica-1.0.0-incubating.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/calcite-core-1.0.0-incubating.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/calcite-linq4j-1.0.0-incubating.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-beanutils-1.9.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-beanutils-core-1.8.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-cli-1.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-codec-1.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-codec-1.8.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-collections-3.2.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-compiler-2.7.6.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-compress-1.4.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-configuration-1.6.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-daemon-1.0.13.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-dbcp-1.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-digester-1.8.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-el-1.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-httpclient-3.0.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-httpclient-3.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-io-2.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-jexl-2.1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-lang-2.6.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-lang3-3.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-logging-1.1.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-math-2.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-math3-3.1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-net-3.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-pool-1.5.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/commons-vfs2-2.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/curator-client-2.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/curator-client-2.7.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/curator-framework-2.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/curator-framework-2.7.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/curator-recipes-2.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/curator-recipes-2.7.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/datanucleus-api-jdo-3.2.6.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/datanucleus-core-3.2.10.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/datanucleus-rdbms-3.2.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/derby-10.11.1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/eigenbase-properties-1.1.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/fastutil-6.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/findbugs-annotations-1.3.9-1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-avro-source-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-dataset-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-file-channel-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-hdfs-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-hive-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-irc-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-jdbc-channel-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-jms-source-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-kafka-channel-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-kafka-source-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-auth-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-config-filter-api-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-configuration-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-core-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-elasticsearch-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-embedded-agent-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-environment-variable-config-filter-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-external-process-config-filter-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-hadoop-credential-store-config-filter-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-hbase-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-kafka-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-log4jappender-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-morphline-solr-sink-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-node-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-ng-sdk-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-scribe-source-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-spillable-memory-channel-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-taildir-source-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-thrift-source-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-tools-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/flume-twitter-source-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/geronimo-annotation_1.0_spec-1.1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/geronimo-jaspic_1.0_spec-1.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/geronimo-jta_1.1_spec-1.1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/groovy-all-2.4.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/gson-2.2.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/guava-11.0.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/guava-11.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/guava-14.0.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/guice-3.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/guice-servlet-3.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-annotations-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-ant-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-archive-logs-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-archives-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-auth-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-aws-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-azure-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-azure-datalake-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-common-2.6.0-cdh5.16.1-tests.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-common-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-datajoin-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-distcp-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-extras-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-gridmix-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-hdfs-2.6.0-cdh5.16.1-tests.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-hdfs-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-hdfs-nfs-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-app-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-common-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-core-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-hs-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-hs-plugins-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.16.1-tests.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-nativetask-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-mapreduce-examples-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-nfs-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-openstack-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-rumen-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-sls-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-streaming-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-api-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-applications-distributedshell-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-client-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-common-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-registry-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-server-applicationhistoryservice-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-server-common-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-server-nodemanager-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-server-resourcemanager-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-server-tests-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hadoop-yarn-server-web-proxy-2.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hamcrest-core-1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hamcrest-core-1.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hbase-annotations-1.2.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hbase-client-1.2.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hbase-common-1.2.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hbase-hadoop-compat-1.2.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hbase-hadoop2-compat-1.2.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hbase-protocol-1.2.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hbase-server-1.2.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/high-scale-lib-1.1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-accumulo-handler-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-ant-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-beeline-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-classification-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-cli-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-common-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-contrib-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-exec-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-hbase-handler-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-hwi-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-jdbc-1.1.0-cdh5.16.1-standalone.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-jdbc-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-metastore-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-serde-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-service-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-shims-0.23-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-shims-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-shims-common-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-shims-scheduler-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hive-testutils-1.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/htrace-core-3.2.0-incubating.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/htrace-core4-4.0.1-incubating.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/httpclient-4.2.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/httpcore-4.2.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/hue-plugins-3.9.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/irclib-1.10.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/ivy-2.0.0-rc2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jackson-annotations-2.2.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jackson-core-2.2.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jackson-core-asl-1.8.10.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jackson-databind-2.2.3-cloudera.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jackson-jaxrs-1.8.10.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jackson-mapper-asl-1.8.10-cloudera.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jackson-xc-1.8.10.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jamon-runtime-2.3.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/janino-2.7.6.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jasper-compiler-5.5.23.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jasper-runtime-5.5.23.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/java-xmlbuilder-0.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/javax.inject-1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jaxb-api-2.2.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jaxb-impl-2.2.3-1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jcommander-1.32.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jdo-api-3.0.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jersey-client-1.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jersey-core-1.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jersey-guice-1.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jersey-json-1.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jersey-server-1.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jets3t-0.6.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jets3t-0.9.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jettison-1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jetty-6.1.26.cloudera.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jetty-all-7.6.0.v20120127.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jetty-all-server-7.6.0.v20120127.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jetty-util-6.1.26.cloudera.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jline-2.11.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jline-2.12.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/joda-time-1.6.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/joda-time-2.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jopt-simple-5.0.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jpam-1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jsch-0.1.42.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jsp-api-2.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jsr305-1.3.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jsr305-3.0.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/jta-1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/kafka-clients-0.10.2-kafka-2.2.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/kafka_2.10-0.10.2-kafka-2.2.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/kite-data-core-1.0.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/kite-data-hbase-1.0.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/kite-data-hive-1.0.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/kite-hadoop-compatibility-1.0.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/leveldbjni-all-1.8.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/libfb303-0.9.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/libthrift-0.9.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/log4j-1.2.16.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/log4j-1.2.17.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/logredactor-1.0.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/lz4-1.3.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/mail-1.4.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/mapdb-0.9.9.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/maven-scm-api-1.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/maven-scm-provider-svn-commons-1.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/maven-scm-provider-svnexe-1.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/metrics-core-2.2.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/metrics-core-3.0.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/metrics-json-3.0.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/metrics-jvm-3.0.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/microsoft-windowsazure-storage-sdk-0.6.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/mina-core-2.0.4.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/mockito-all-1.8.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/netty-3.10.5.Final.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/netty-3.9.4.Final.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/netty-all-4.0.23.Final.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/okhttp-2.4.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/okio-1.4.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/opencsv-2.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/oro-2.0.8.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/paranamer-2.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-avro-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-cascading-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-column-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-common-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-encoding-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-format-2.1.0-cdh5.16.1-javadoc.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-format-2.1.0-cdh5.16.1-sources.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-format-2.1.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-generator-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-hadoop-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-hadoop-bundle-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-jackson-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-pig-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-pig-bundle-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-protobuf-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-scala_2.10-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-scrooge_2.10-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-test-hadoop2-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-thrift-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/parquet-tools-1.5.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/plexus-utils-1.5.6.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/protobuf-java-2.5.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/regexp-1.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/scala-library-2.10.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/serializer-2.7.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/servlet-api-2.5-20110124.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/servlet-api-2.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/slf4j-api-1.7.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/slf4j-log4j12-1.7.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/snappy-java-1.0.4.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/spark-1.6.0-cdh5.16.1-yarn-shuffle.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/spark-streaming-flume-sink_2.10-1.6.0-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/stax-api-1.0-2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/stax-api-1.0.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/stringtemplate-3.2.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/super-csv-2.2.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/tempus-fugit-1.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/trevni-avro-1.7.6-cdh5.16.1-hadoop2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/trevni-avro-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/trevni-core-1.7.6-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/twitter4j-core-3.0.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/twitter4j-media-support-3.0.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/twitter4j-stream-3.0.3.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/unused-1.0.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/velocity-1.5.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/velocity-1.7.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/xalan-2.7.2.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/xercesImpl-2.9.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/xml-apis-1.3.04.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/xmlenc-0.52.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/xz-1.0.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/zkclient-0.10.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/jars/zookeeper-3.4.5-cdh5.16.1.jar:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/lib/hadoop/LICENSE.txt:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/lib/hadoop/NOTICE.txt
18/12/17 22:12:30 INFO zookeeper.ZooKeeper: Client environment:java.library.path=:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/bin/../lib/hadoop/lib/native:/opt/cloudera/parcels/CDH-5.16.1-1.cdh5.16.1.p0.3/lib/hadoop/lib/native:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
18/12/17 22:12:30 INFO zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
18/12/17 22:12:30 INFO zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
18/12/17 22:12:30 INFO zookeeper.ZooKeeper: Client environment:os.name=Linux
18/12/17 22:12:30 INFO zookeeper.ZooKeeper: Client environment:os.arch=amd64
18/12/17 22:12:30 INFO zookeeper.ZooKeeper: Client environment:os.version=3.10.0-862.3.3.el7.x86_64
18/12/17 22:12:30 INFO zookeeper.ZooKeeper: Client environment:user.name=hdfs
18/12/17 22:12:30 INFO zookeeper.ZooKeeper: Client environment:user.home=/var/lib/hadoop-hdfs
18/12/17 22:12:30 INFO zookeeper.ZooKeeper: Client environment:user.dir=/var/lib/hadoop-hdfs/benchmark-baremetal/src
18/12/17 22:12:30 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=cdh-utility-1.public1.cdhvcn.oraclevcn.com:2181,cdh-master-1.private1.cdhvcn.oraclevcn.com:2181,cdh-master-2.private2.cdhvcn.oraclevcn.com:2181 sessionTimeout=1200000 watcher=org.apache.curator.ConnectionState@5badedc
18/12/17 22:12:30 INFO zookeeper.ClientCnxn: Opening socket connection to server cdh-master-1.private1.cdhvcn.oraclevcn.com/10.0.3.2:2181. Will not attempt to authenticate using SASL (unknown error)
18/12/17 22:12:30 INFO zookeeper.ClientCnxn: Socket connection established to cdh-master-1.private1.cdhvcn.oraclevcn.com/10.0.3.2:2181, initiating session
18/12/17 22:12:30 INFO zookeeper.ClientCnxn: Session establishment complete on server cdh-master-1.private1.cdhvcn.oraclevcn.com/10.0.3.2:2181, sessionid = 0x267bc7ae61e028a, negotiated timeout = 40000
18/12/17 22:12:30 INFO state.ConnectionStateManager: State change: CONNECTED
18/12/17 22:12:31 INFO log.PerfLogger: </PERFLOG method=acquireReadWriteLocks start=1545084750338 end=1545084751417 duration=1079 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO log.PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO ql.Driver: Executing command(queryId=): USE DEFAULT
18/12/17 22:12:31 INFO log.PerfLogger: </PERFLOG method=TimeToSubmit start=1545084749918 end=1545084751422 duration=1504 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO log.PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO ql.Driver: Starting task [Stage-0:DDL] in serial mode
18/12/17 22:12:31 INFO log.PerfLogger: </PERFLOG method=runTasks start=1545084751422 end=1545084751432 duration=10 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO log.PerfLogger: </PERFLOG method=Driver.execute start=1545084751417 end=1545084751433 duration=16 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO ql.Driver: Completed executing command(queryId=); Time taken: 0.016 seconds
18/12/17 22:12:31 INFO ql.Driver: OK
18/12/17 22:12:31 INFO log.PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO log.PerfLogger: </PERFLOG method=releaseLocks start=1545084751433 end=1545084751433 duration=0 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO log.PerfLogger: </PERFLOG method=Driver.run start=1545084749917 end=1545084751433 duration=1516 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO log.PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO log.PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO log.PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO ql.Driver: Compiling command(queryId=): DROP TABLE IF EXISTS uservisits
18/12/17 22:12:31 INFO log.PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO log.PerfLogger: </PERFLOG method=parse start=1545084751613 end=1545084751614 duration=1 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO log.PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO ql.Driver: Semantic Analysis Completed
18/12/17 22:12:31 INFO log.PerfLogger: </PERFLOG method=semanticAnalyze start=1545084751615 end=1545084751656 duration=41 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
18/12/17 22:12:31 INFO log.PerfLogger: </PERFLOG method=compile start=1545084751612 end=1545084751656 duration=44 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO log.PerfLogger: </PERFLOG method=compile start=1545084751612 end=1545084751656 duration=44 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO ql.Driver: Completed compiling command(queryId=); Time taken: 0.044 seconds
18/12/17 22:12:31 INFO log.PerfLogger: <PERFLOG method=acquireReadWriteLocks from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO log.PerfLogger: </PERFLOG method=acquireReadWriteLocks start=1545084751657 end=1545084751657 duration=0 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO log.PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO ql.Driver: Executing command(queryId=): USE DEFAULT
18/12/17 22:12:31 INFO log.PerfLogger: </PERFLOG method=TimeToSubmit start=1545084751612 end=1545084751657 duration=45 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO log.PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO ql.Driver: Starting task [Stage-0:DDL] in serial mode
18/12/17 22:12:31 ERROR metadata.Hive: Table uservisits not found: DEFAULT.uservisits table not found
18/12/17 22:12:31 INFO log.PerfLogger: </PERFLOG method=runTasks start=1545084751657 end=1545084751671 duration=14 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO log.PerfLogger: </PERFLOG method=Driver.execute start=1545084751657 end=1545084751671 duration=14 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO ql.Driver: Completed executing command(queryId=); Time taken: 0.014 seconds
18/12/17 22:12:31 INFO ql.Driver: OK
18/12/17 22:12:31 INFO log.PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO log.PerfLogger: </PERFLOG method=releaseLocks start=1545084751672 end=1545084751672 duration=0 from=org.apache.hadoop.hive.ql.Driver>
18/12/17 22:12:31 INFO log.PerfLogger: </PERFLOG method=Driver.run start=1545084751612 end=1545084751672 duration=60 from=org.apache.hadoop.hive.ql.Driver>
NoViableAltException(26@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser.type(HiveParser.java:39361)
	at org.apache.hadoop.hive.ql.parse.HiveParser.colType(HiveParser.java:39126)
	at org.apache.hadoop.hive.ql.parse.HiveParser.columnNameType(HiveParser.java:38826)
	at org.apache.hadoop.hive.ql.parse.HiveParser.columnNameTypeList(HiveParser.java:37049)
	at org.apache.hadoop.hive.ql.parse.HiveParser.createTableStatement(HiveParser.java:5059)
	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2559)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1591)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1067)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:205)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:170)
	at org.apache.spark.sql.hive.HiveQl$.getAst(HiveQl.scala:276)
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:303)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:295)
	at org.apache.spark.sql.hive.HiveQLDialect$$anonfun$parse$1.apply(HiveContext.scala:66)
	at org.apache.spark.sql.hive.HiveQLDialect$$anonfun$parse$1.apply(HiveContext.scala:66)
	at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1.apply(ClientWrapper.scala:293)
	at org.apache.spark.sql.hive.client.ClientWrapper.liftedTree1$1(ClientWrapper.scala:239)
	at org.apache.spark.sql.hive.client.ClientWrapper.retryLocked(ClientWrapper.scala:238)
	at org.apache.spark.sql.hive.client.ClientWrapper.withHiveState(ClientWrapper.scala:281)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:65)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:211)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:211)
	at org.apache.spark.sql.execution.SparkSQLParser$$anonfun$org$apache$spark$sql$execution$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at org.apache.spark.sql.execution.SparkSQLParser$$anonfun$org$apache$spark$sql$execution$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:113)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:208)
	at org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:208)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:43)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:231)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:334)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:829)
	at com.intel.hibench.sparkbench.sql.ScalaSparkSQLBench$$anonfun$main$1.apply(ScalaSparkSQLBench.scala:43)
	at com.intel.hibench.sparkbench.sql.ScalaSparkSQLBench$$anonfun$main$1.apply(ScalaSparkSQLBench.scala:41)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at com.intel.hibench.sparkbench.sql.ScalaSparkSQLBench$.main(ScalaSparkSQLBench.scala:41)
	at com.intel.hibench.sparkbench.sql.ScalaSparkSQLBench.main(ScalaSparkSQLBench.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:730)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Exception in thread "main" org.apache.spark.sql.AnalysisException: cannot recognize input near 'archWord' 'STRING' ',' in column type; line 2 pos 0
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:318)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:295)
	at org.apache.spark.sql.hive.HiveQLDialect$$anonfun$parse$1.apply(HiveContext.scala:66)
	at org.apache.spark.sql.hive.HiveQLDialect$$anonfun$parse$1.apply(HiveContext.scala:66)
	at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1.apply(ClientWrapper.scala:293)
	at org.apache.spark.sql.hive.client.ClientWrapper.liftedTree1$1(ClientWrapper.scala:239)
	at org.apache.spark.sql.hive.client.ClientWrapper.retryLocked(ClientWrapper.scala:238)
	at org.apache.spark.sql.hive.client.ClientWrapper.withHiveState(ClientWrapper.scala:281)
	at org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:65)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:211)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:211)
	at org.apache.spark.sql.execution.SparkSQLParser$$anonfun$org$apache$spark$sql$execution$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)
	at org.apache.spark.sql.execution.SparkSQLParser$$anonfun$org$apache$spark$sql$execution$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:113)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)
	at org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:208)
	at org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:208)
	at org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:43)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:231)
	at org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:334)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:829)
	at com.intel.hibench.sparkbench.sql.ScalaSparkSQLBench$$anonfun$main$1.apply(ScalaSparkSQLBench.scala:43)
	at com.intel.hibench.sparkbench.sql.ScalaSparkSQLBench$$anonfun$main$1.apply(ScalaSparkSQLBench.scala:41)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at com.intel.hibench.sparkbench.sql.ScalaSparkSQLBench$.main(ScalaSparkSQLBench.scala:41)
	at com.intel.hibench.sparkbench.sql.ScalaSparkSQLBench.main(ScalaSparkSQLBench.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:730)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
18/12/17 22:12:31 INFO spark.SparkContext: Invoking stop() from shutdown hook
18/12/17 22:12:31 INFO zookeeper.ZooKeeper: Session: 0x267bc7ae61e028a closed
18/12/17 22:12:31 INFO zookeeper.ClientCnxn: EventThread shut down
18/12/17 22:12:31 INFO CuratorFrameworkSingleton: Closing ZooKeeper client.
18/12/17 22:12:31 INFO ui.SparkUI: Stopped Spark web UI at http://10.0.0.2:4040
18/12/17 22:12:31 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread
18/12/17 22:12:31 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors
18/12/17 22:12:31 INFO cluster.YarnClientSchedulerBackend: Asking each executor to shut down
18/12/17 22:12:31 INFO cluster.YarnClientSchedulerBackend: Stopped
18/12/17 22:12:31 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/12/17 22:12:31 INFO storage.MemoryStore: MemoryStore cleared
18/12/17 22:12:31 INFO storage.BlockManager: BlockManager stopped
18/12/17 22:12:31 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
18/12/17 22:12:31 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/12/17 22:12:32 INFO spark.SparkContext: Successfully stopped SparkContext
18/12/17 22:12:32 INFO util.ShutdownHookManager: Shutdown hook called
18/12/17 22:12:32 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-45e00120-76b4-4c30-adef-d9c05a52b8c7
18/12/17 22:12:32 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-9fee23e0-45b6-40e0-bf9c-c4e19c56b679
18/12/17 22:12:32 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
18/12/17 22:12:32 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
